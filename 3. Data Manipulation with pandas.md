# Introducing DataFrames
## DataFrames
>**Pandas is based on NumPy and Matplotlib packages**
- In pandas, the rectangular data is stored as dataFrame object
- Every column in the dataFrame contain the same data type but different column can contain different data types

>**Ways of Exploring the data frame**
- .head() - display first five rows of the dataFrame
- .info() - displays names of columns, the data type and whether it has any missing columns
- .shape  - displays the dimension of the dataset into (rows, columns). Note that, this is an attribute and not a method hence the paranthesis is not required
- .describe() - computes some summary statstics for the columns. Provides quick overview of the numeric clumns
> Dataframes consists of three components, accessible through attributes
- .values - contains the data values reprenseted in 2D- NumPy array 1 array for each row 
- .columns - contains columns names
- .index - contains row numbers or row names

>**Exapmle - Pandas Dataset exploration**
```python
# Print the head of the homelessness data
print(homelessness.head())

# Print information about homelessness
print(homelessness.info())

# Print the shape of homelessness
print(homelessness.shape)

# Print a description of homelessness
print(homelessness.describe())

# Import pandas using the alias pd
import pandas as pd

# Print the values of homelessness
print(homelessness.values)

# Print the column index of homelessness
print(homelessness.columns)

# Print the row index of homelessness
print(homelessness.index)
```

## Sorting and Subsetting
### Sorting

>**Changing order of rows by sorting**
```python
df.sort_values("column_name") #Will sort datafrmae in the ascending order
df.sort_values("column_name", ascending=Fales) # Will sort int the descending order

# We can sort by multiple variables by passing them as a list
df.sort_values(["column_name1","column_name2"])
#while we need to sort order to be different, pass a list of bollenasto asending paramerters
df.sort_values(["column_name1","column_name2"], ascending = [True, False])
```

### Subsetting
>**Subsetting can be done or rows or columns**

#### Subsetting Columns

>**We may want to zoom in to just a few columns or multiple**
```python
df["column_name"] #displays the content of only 1 column
df[["column_name1","column_name2"]] # Displaying multiple columns, pass the list of column names

# You can provide a seperate list of columns as a variable and then perform subsetting

cols_to_subset = ["column_name1","column_name2"]
df[cols_to_subset]

```
#### Subsetting rows

>**We may want to zoom in to just a few rows or multiple**

```python
df["columns_name"] > value # Returns a boolean. Use this logical condition inside []
df[df["column_name"] > value]
```

>**Subsetting based on text data**
```python
df[df["column_name"] == "value"]
```
>**subsetting based on dates**
```python
df[df["date_column_name"] < "2015-01-01"]
```
>**subsetting based on multiple conditions using logical operators**
```python
is_lab = dogs["breed"] == "Labrador"
is_brown = dogs["color"] == "Brown"
dogs[is_lab & is_brown]

# this can be done in one line of code**

dogs[(dogs["breed"] == "Labrador") & (dogs["color"] == "Brown")]
```
>**Subsetting using.isin()**
```python
is_black_or_brown = dogs["color"].isin(["Black","Brown"])
dogs[is_black_or_brown]
```

>**Exapmle Sorting and Subsetting**
```python
# Sort homelessness by individuals
homelessness_ind = homelessness.sort_values("individuals")

# Print the top few rows
print(homelessness_ind.head())

#########
# Sort homelessness by descending family members
homelessness_fam = homelessness.sort_values("family_members" , ascending = False)

# Print the top few rows
print(homelessness_fam.head())

#########
# Sort homelessness by region, then descending family members
homelessness_reg_fam = homelessness.sort_values(["region", "family_members"], ascending=[True, False])

# Print the top few rows
print(homelessness_reg_fam.head())

#######################
# Select the individuals column
individuals = homelessness["individuals"]

# Print the head of the result
print(individuals.head())

#########
# Select the state and family_members columns
state_fam = homelessness[["state","family_members"]]

# Print the head of the result
print(state_fam.head())

#########
# Select only the individuals and state columns, in that order
ind_state = homelessness[["individuals","state"]]

# Print the head of the result
print(ind_state.head())

################ Filtering rows
# Filter for rows where individuals is greater than 10000
ind_gt_10k = homelessness[homelessness["individuals"] > 10000]

# See the result
print(ind_gt_10k)

#########
# Filter for rows where region is Mountain
mountain_reg = homelessness[homelessness["region"] == "Mountain"]

# See the result
print(mountain_reg)

#########
# Filter for rows where family_members is less than 1000 
# and region is Pacific
fam_lt_1k_pac = homelessness[(homelessness["family_members"] < 1000) & (homelessness["region"] == "Pacific")]

# See the result
print(fam_lt_1k_pac)


#########
# Subset for rows in South Atlantic or Mid-Atlantic regions
south_mid_atlantic = homelessness[homelessness["region"].isin(["South Atlantic", "Mid-Atlantic"])]

# See the result
print(south_mid_atlantic)

#####
# The Mojave Desert states
canu = ["California", "Arizona", "Nevada", "Utah"]

# Filter for rows in the Mojave Desert states
mojave_homelessness = homelessness[homelessness["state"].isin(canu)]

# See the result
print(mojave_homelessness)
```
## New Columns
### Adding New Columns
>**syntax**
>df["new_column_name"] = df["old_column_name"] with some calculation

```python
dogs["bmi"] = dogs["weight_kg"] / dogs["height_m"] ** 2
```
>**Exapmle:- Adding new columns**
```python
# Add total col as sum of individuals and family_members
homelessness["total"] = homelessness["individuals"] + homelessness["family_members"]

# Add p_individuals col as proportion of total that are individuals
homelessness["p_individuals"] = homelessness["individuals"] / homelessness["total"]

# See the result
print(homelessness)

##############
# Create indiv_per_10k col as homeless individuals per 10k state pop
homelessness["indiv_per_10k"] = 10000 * homelessness["individuals"] / homelessness["state_pop"] 

# Subset rows for indiv_per_10k greater than 20
high_homelessness = homelessness[homelessness["indiv_per_10k"] > 20]

# Sort high_homelessness by descending indiv_per_10k
high_homelessness_srt = high_homelessness.sort_values("indiv_per_10k", ascending = False)

# From high_homelessness_srt, select the state and indiv_per_10k cols
result = high_homelessness_srt[["state", "indiv_per_10k"]]

# See the result
print(result)
```
# Summary Stattistics - Aggregating the data

## Summarizing the numerical data
- .mean() to identify the center of your data
- .median(), .mode()
- .min(), .max()
- .var(), .std()
- .sum()
- .quantile()

## Summarizing the Date columns
- similar methods as of numerical data
```python
dogs["date_birth"].min() # Oldest dog
dogs["date_birth"].max() # Youngest dog
```
- .agg() #or aggregate method allows us to compute custom summart statistics
```python
def pct30(column):
  return column.quantile(0.3)
  
dogs["weight_kg"].agg(pct30) #calculates 30th percentil of the dogs weight
```
>**Agg can be used for more than one columns**
```python
dogs[["weight_kg","height_cm"]].agg(pct30)
```

>**We can pass multiple custom summaries as well**
```python
def pct40(column):
  return column.quantile(0.4)
  
dogs["weight_kg"].agg([pct30, pct40])

```
>**Cumulative statistics - return entire column of the dataframe rather than just one summary number**
- Cumulative sum using the method .cumsum()

```python
dogs["weight_kg"].cumsum()
```
- .cummax()
- .cummin()
- .cumprod()

>**Examples - Mean and median**
```python

# Print the head of the sales DataFrame
print(sales.head())

# Print the info about the sales DataFrame
print(sales.info())

# Print the mean of weekly_sales
print(sales["weekly_sales"].mean())

# Print the median of weekly_sales
print(sales["weekly_sales"].median())

```

>**Example - Summarizing the dates**
```python
# Print the maximum of the date column
print(sales["date"].max())

# Print the minimum of the date column
print(sales["date"].min())
```
>**Exapmle - Efficient Summaries**
```python
# A custom IQR function
def iqr(column):
    return column.quantile(0.75) - column.quantile(0.25)
    
# Print IQR of the temperature_c column
print(sales["temperature_c"].agg(iqr))

>> Output
16.583333333333336

# Update to print IQR and median of temperature_c, fuel_price_usd_per_l, & unemployment
print(sales[["temperature_c", "fuel_price_usd_per_l", "unemployment"]].agg(iqr))

>> Output
temperature_c           16.583
fuel_price_usd_per_l     0.073
unemployment             0.565
dtype: float64

# Update to print IQR and median of temperature_c, fuel_price_usd_per_l, & unemployment
print(sales[["temperature_c", "fuel_price_usd_per_l", "unemployment"]].agg([iqr, np.median]))

>> Output
        temperature_c  fuel_price_usd_per_l  unemployment
iqr            16.583                 0.073         0.565
median         16.967                 0.743         8.099
```

>**Example - Cumulative Statistics**
```python
# Sort sales_1_1 by date
sales_1_1 = sales_1_1.sort_values("date")

# Get the cumulative sum of weekly_sales, add as cum_weekly_sales col
sales_1_1["cum_weekly_sales"] = sales_1_1["weekly_sales"].cumsum()

# Get the cumulative max of weekly_sales, add as cum_max_sales col
sales_1_1["cum_max_sales"] = sales_1_1["weekly_sales"].cummax()

# See the columns you calculated
print(sales_1_1[["date", "weekly_sales", "cum_weekly_sales", "cum_max_sales"]])

>> Output
            date  weekly_sales  cum_weekly_sales  cum_max_sales
0     2010-02-05      24924.50         2.492e+04       24924.50
1894  2010-02-05      21654.54         4.658e+04       24924.50
4271  2010-02-05     232558.51         2.791e+05      232558.51
4283  2010-02-05      56702.80         3.358e+05      232558.51
4296  2010-02-05         12.00         3.359e+05      232558.51
...          ...           ...               ...            ...
```
## Counting - Summarizing categorical data

>**Removing duplicates**
```python
vet_visits.drop_duplicates(subset = "name") # Removes dogs records with duplicate names
#THis might remove records with same dog names. So need to add more columns for identifying duplicates
vet_visits.drop_duplicates(subset = ["name", "breed"]) #Pass a list of column names to subset arguments

```
>**Count values**
```
unique_dogs["breed"].value_counts()
#We can also sort based on count values
unique_dogs["breed"].value_counts(sort= True)
unique_dogs["breed"].value_counts(normalize= True) #Used to convert counts into proportions of the total
```
> **Example - Dropping Duplicates**
```python
# Drop duplicate store/type combinations
store_types = sales.drop_duplicates(subset = ["store", "type"])
print(store_types.head())

>> Output
      store type  department       date  weekly_sales  is_holiday  temperature_c  fuel_price_usd_per_l  unemployment
0         1    A           1 2010-02-05      24924.50       False          5.728                 0.679         8.106
901       2    A           1 2010-02-05      35034.06       False          4.550                 0.679         8.324
1798      4    A           1 2010-02-05      38724.42       False          6.533                 0.686         8.623
2699      6    A           1 2010-02-05      25619.00       False          4.683                 0.679         7.259
3593     10    B           1 2010-02-05      40212.84       False         12.411                 0.782         9.765

# Drop duplicate store/department combinations
store_depts = sales.drop_duplicates(subset = ["store", "department"])
print(store_depts.head())

>> Output
    store type  department       date  weekly_sales  is_holiday  temperature_c  fuel_price_usd_per_l  unemployment
0       1    A           1 2010-02-05      24924.50       False          5.728                 0.679         8.106
12      1    A           2 2010-02-05      50605.27       False          5.728                 0.679         8.106
24      1    A           3 2010-02-05      13740.12       False          5.728                 0.679         8.106
36      1    A           4 2010-02-05      39954.04       False          5.728                 0.679         8.106
48      1    A           5 2010-02-05      32229.38       False          5.728                 0.679         8.106

# Subset the rows where is_holiday is True and drop duplicate dates
holiday_dates = sales[sales["is_holiday"] == True].drop_duplicates(subset="date")

# Print date col of holiday_dates
print(holiday_dates)

>> Output
      store type  department       date  weekly_sales  is_holiday  temperature_c  fuel_price_usd_per_l  unemployment
498       1    A          45 2010-09-10         11.47        True         25.939                 0.678         7.787
691       1    A          77 2011-11-25       1431.00        True         15.633                 0.855         7.866
2315      4    A          47 2010-02-12        498.00        True         -1.756                 0.680         8.623
6735     19    A          39 2012-09-07         13.41        True         22.333                 1.077         8.193
6810     19    A          47 2010-12-31       -449.00        True         -1.861                 0.881         8.067
6815     19    A          47 2012-02-10         15.00        True          0.339                 1.011         7.943
6820     19    A          48 2011-09-09        197.00        True         20.156                 1.038         7.806

```

>**Example - Counting categorical variables**
```python
# Count the number of stores of each type
store_counts = store_types["type"].value_counts()
print(store_counts)

>> Output
A    11
B     1
Name: type, dtype: int64

# Get the proportion of stores of each type
store_props = store_types["type"].value_counts(normalize = True)
print(store_props)

>> Output
A    0.917
B    0.083
Name: type, dtype: float64

# Count the number of each department number and sort
dept_counts_sorted = store_depts["department"].value_counts(sort = True)
print(dept_counts_sorted)

>> Output
1     12
55    12
72    12
71    12
67    12
      ..
37    10
48     8
50     6
39     4
43     2
Name: department, Length: 80, dtype: int64

# Get the proportion of departments of each number and sort
dept_props_sorted = store_depts["department"].value_counts(sort=True, normalize=True)
print(dept_props_sorted)

>> Output
1     0.013
55    0.013
72    0.013
71    0.013
67    0.013
      ...  
37    0.011
48    0.009
50    0.006
39    0.004
43    0.002
Name: department, Length: 80, dtype: float64
```

## Grouped Summary Statistics

>**Summaries by Groups**
```python
dogs[dogs["color"] == "Black"]["weight_kg"].mean()
dogs[dogs["color"] == "Brown"]["weight_kg"].mean()
dogs[dogs["color"] == "White"]["weight_kg"].mean()
dogs[dogs["color"] == "Gray"]["weight_kg"].mean()
dogs[dogs["color"] == "Tan"]["weight_kg"].mean()

#Instead of writing code multiple times, groupBy method can be used
dogs.groupby("color)["weight_kg"].mean()
```

>**Multiple Summaries by groups**
```python
dogs.groupby("color")["weight_kg"].agg([min, max, sum])
```

>**Grouping by multiple variables**
```python
dogs.groupby(["color","breed"])["weight_kg"].mean()
```

>**Group by and aggregate by multiple columns**
```python
dogs.groupby(["color","breed"])[["weight_kg", "height_cm"]].mean()
```

>**Example - Subset and then add**
```python
# Calc total weekly sales
sales_all = sales["weekly_sales"].sum()
print(sales_all)

# Subset for type A stores, calc total weekly sales
sales_A = sales[sales["type"] == "A"]["weekly_sales"].sum()
print(sales_A)

# Subset for type B stores, calc total weekly sales
sales_B = sales[sales["type"] == "B"]["weekly_sales"].sum()

# Subset for type C stores, calc total weekly sales
sales_C = sales[sales["type"] == "C"]["weekly_sales"].sum()

# Get proportion for each type
sales_propn_by_type = [sales_A, sales_B, sales_C] / sales_all
print(sales_propn_by_type)

>> Output
256894718.89999998
233716315.01
[0.9097747 0.0902253 0.       ]
```

>**Example 2 - Calculations with GroupBy()**
```
# Group by type; calc total weekly sales
sales_by_type = sales.groupby("type")["weekly_sales"].sum()
print(sales_by_type)
# Get proportion for each type
sales_propn_by_type = sales_by_type / sum(sales_by_type)
print(sales_propn_by_type)

>> Output
type
A    2.337e+08
B    2.318e+07
Name: weekly_sales, dtype: float64

type
A    0.91
B    0.09
Name: weekly_sales, dtype: float64
```

>**Example3 - Multiple groupings**
```python
# From previous step
sales_by_type = sales.groupby("type")["weekly_sales"].sum()

# Group by type and is_holiday; calc total weekly sales
sales_by_type_is_holiday = sales.groupby(["type","is_holiday"])["weekly_sales"].sum()
print(sales_by_type_is_holiday)

>> Output
type  is_holiday
A     False         2.337e+08
      True          2.360e+04
B     False         2.318e+07
      True          1.621e+03
Name: weekly_sales, dtype: float64
```
>**Example 4 - Multiple Group Summaries**
```python
# Import numpy with the alias np
import numpy as np

# For each store type, aggregate weekly_sales: get min, max, mean, and median
sales_stats = sales.groupby("type")["weekly_sales"].agg([np.min, np.max, np.mean, np.median])

# Print sales_stats
print(sales_stats)

# For each store type, aggregate unemployment and fuel_price_usd_per_l: get min, max, mean, and median
unemp_fuel_stats = sales.groupby("type")[["unemployment","fuel_price_usd_per_l"]].agg([np.min, np.max, np.mean, np.median])

# Print unemp_fuel_stats
print(unemp_fuel_stats)

>> Output

        amin       amax       mean    median
type                                        
A    -1098.0  293966.05  23674.667  11943.92
B     -798.0  232558.51  25696.678  13336.08
     unemployment                      fuel_price_usd_per_l                     
             amin   amax   mean median                 amin   amax   mean median
type                                                                            
A           3.879  8.992  7.973  8.067                0.664  1.107  0.745  0.735
B           7.170  9.765  9.279  9.199                0.760  1.108  0.806  0.803
```

## Pivot Tables - Another way of calculating grouped stats
>**Group by to Pivot tables**
```python
dogs.groupby("color")["weight_kg"].mean()
#Using Pivot tables
dogs.pivot_table(values="weight_kg", index="color") #By Default it calculates the mean
#aggfunc argument calucltaes other stas
dogs.pivot_table(values="weight_kg", index="color", aggfunc = np.median)

#Multiple summary stats. pass list of functions as list
dogs.pivot_table(values="weight_kg", index="color", aggfunc = [np.mean, np.median])
```
>**Pivot on two variables**
```python
dogs.groupby(["color","breed"])["weight_kg"].mean()

#Using Pivot
dogs.pivot_table(values="weight_kg", index="color", columns="breed") #To group by two variables, pass second variable in the column argument
#This will result in NaN if the value of the combination doesnt exist. We can fill nas to 0 
dogs.pivot_table(values="weight_kg", index="color", columns="breed", fill_value = 0)

#To get group total stats, set margins = True. The last rwo and last value will be summary of totals. 
#Not including missing values those are filled with zeros
dogs.pivot_table(values="weight_kg", index="color", columns="breed", fill_value = 0, margins=True)
```
>**Example1 -  Pivoting on 1 variable**
```python
# Pivot for mean weekly_sales for each store type
mean_sales_by_type = sales.pivot_table(values="weekly_sales", index="type")

# Print mean_sales_by_type
print(mean_sales_by_type)

>> Output
      weekly_sales
type              
A        23674.667
B        25696.678

# Import NumPy as np
import numpy as np

# Pivot for mean and median weekly_sales for each store type
mean_med_sales_by_type = sales.pivot_table(values="weekly_sales", index="type", aggfunc=[np.mean, np.median])

# Print mean_med_sales_by_type
print(mean_med_sales_by_type)

>> Output
             mean       median
     weekly_sales weekly_sales
type                          
A       23674.667     11943.92
B       25696.678     13336.08


# Pivot for mean weekly_sales by store type and holiday 
mean_sales_by_type_holiday = sales.pivot_table(values="weekly_sales", index="type", columns="is_holiday")

# Print mean_sales_by_type_holiday
print(mean_sales_by_type_holiday)

>>Output
is_holiday      False     True
type                          
A           23768.584  590.045
B           25751.981  810.705
```

>**Exapmle 2 -Filling Missing Values to 0**
>
```python
# Print mean weekly_sales by department and type; fill missing values with 0
print(sales.pivot_table(values="weekly_sales", index="department", columns="type", fill_value = 0))

>> Output
type                 A           B
department                        
1            30961.725   44050.627
2            67600.159  112958.527
3            17160.003   30580.655
4            44285.399   51219.654
5            34821.011   63236.875

# Print the mean weekly_sales by department and type; fill missing values with 0s; sum all rows and cols
print(sales.pivot_table(values="weekly_sales", index="department", columns="type", fill_value = 0, margins=all))

>> Output
type                A           B        All
department                                  
1           30961.725   44050.627  32052.467
2           67600.159  112958.527  71380.023
3           17160.003   30580.655  18278.391
4           44285.399   51219.654  44863.254
5           34821.011   63236.875  37189.000
...               ...         ...        ...
96          21367.043    9528.538  20337.608
97          28471.267    5828.873  26584.401
98          12875.423     217.428  11820.590
99            379.124       0.000    379.124
All         23674.667   25696.678  23843.950

[81 rows x 3 columns]
```

# Explicit Indexes
>**Setting a column as an index**
```python
dogs_ind = dogs.set_index("name")
```
>**Removing a column as an index**
```python
dogs_ind.reset_index() #The name column is removed from index and assigned basck as a column
```

>**Dropping an index**
```python
dogs_ind.reset_index(drop=True) # Index is removed and also the name column is dropped because it was the part of the index
``` 

>**Indexes makes subsetting simpler**
```python
#without an index
dogs[dogs["name"].isin(["Bella","Stella"])]

#with index we can use .loc
dogs_ind.loc[["Bella","Stella"]]
```
>**Index values don't need to be unque**
>There can be two same values in the indexes
Subsetting on duplicate index will return multiple values

>**Multi-level index a.k.a. Hierarchical indexes**
```python
dogs_ind3 = dogs.set_index(["breed", "color"])
#The inner level of index, here, color is nested within the outer level of index
```

>**Subset Outer level index with list**
```python
dogs_ind3.loc[["Labrador","Chihuahua"]]
```

>**Subset inner levels with a list of tuples**
```python
dogs_ind3.loc[[("Labrador", "Brown") , ("Chihuahua", "Tan")]]
```

>**Sorting based on indexes**
```python
dogs_ind3.sort_index() #By default it will sort all indexes from Outer level to inner level
```
>**You can control the sorting order by passing lists**
```python
dogs_ind3.sort_index(level=["color", "breed"], ascending=[True, False])
```

>**Example1 - Setting and removing indexes**
```python
# Look at temperatures
print(temperatures)

# Set the index of temperatures to city
temperatures_ind = temperatures.set_index("city")

# Look at temperatures_ind
print(temperatures_ind)

>>Output

             date        country  avg_temp_c
city                                         
Abidjan 2000-01-01  Côte D'Ivoire      27.293
Abidjan 2000-02-01  Côte D'Ivoire      27.685
Abidjan 2000-03-01  Côte D'Ivoire      29.061
Abidjan 2000-04-01  Côte D'Ivoire      28.162
Abidjan 2000-05-01  Côte D'Ivoire      27.547
...            ...            ...         ...
Xian    2013-05-01          China      18.979
Xian    2013-06-01          China      23.522
Xian    2013-07-01          China      25.251
Xian    2013-08-01          China      24.528
Xian    2013-09-01          China         NaN

[16500 rows x 3 columns]

# Reset the temperatures_ind index, keeping its contents
print(temperatures_ind.reset_index())

>>Output
          city       date        country  avg_temp_c
0      Abidjan 2000-01-01  Côte D'Ivoire      27.293
1      Abidjan 2000-02-01  Côte D'Ivoire      27.685
2      Abidjan 2000-03-01  Côte D'Ivoire      29.061
3      Abidjan 2000-04-01  Côte D'Ivoire      28.162
4      Abidjan 2000-05-01  Côte D'Ivoire      27.547
...        ...        ...            ...         ...
16495     Xian 2013-05-01          China      18.979
16496     Xian 2013-06-01          China      23.522
16497     Xian 2013-07-01          China      25.251
16498     Xian 2013-08-01          China      24.528
16499     Xian 2013-09-01          China         NaN

[16500 rows x 4 columns]

# Reset the temperatures_ind index, dropping its contents
print(temperatures_ind.reset_index(drop=True))

>>Output

            date        country  avg_temp_c
0     2000-01-01  Côte D'Ivoire      27.293
1     2000-02-01  Côte D'Ivoire      27.685
2     2000-03-01  Côte D'Ivoire      29.061
3     2000-04-01  Côte D'Ivoire      28.162
4     2000-05-01  Côte D'Ivoire      27.547
...          ...            ...         ...
16495 2013-05-01          China      18.979
16496 2013-06-01          China      23.522
16497 2013-07-01          China      25.251
16498 2013-08-01          China      24.528
16499 2013-09-01          China         NaN

[16500 rows x 3 columns]
```
>**Example2 - Subsetting with .loc[]**
```python
# Make a list of cities to subset on
cities = ["Moscow", "Saint Petersburg"]

# Subset temperatures using square brackets
print(temperatures[temperatures["city"].isin(cities)])

>>Output
            date              city country  avg_temp_c
10725 2000-01-01            Moscow  Russia      -7.313
10726 2000-02-01            Moscow  Russia      -3.551
10727 2000-03-01            Moscow  Russia      -1.661
10728 2000-04-01            Moscow  Russia      10.096
10729 2000-05-01            Moscow  Russia      10.357
...          ...               ...     ...         ...
13360 2013-05-01  Saint Petersburg  Russia      12.355
13361 2013-06-01  Saint Petersburg  Russia      17.185
13362 2013-07-01  Saint Petersburg  Russia      17.234
13363 2013-08-01  Saint Petersburg  Russia      17.153
13364 2013-09-01  Saint Petersburg  Russia         NaN

[330 rows x 4 columns]

# Subset temperatures_ind using .loc[]
print(temperatures_ind.loc[cities])

>>Output
                       date country  avg_temp_c
city                                           
Moscow           2000-01-01  Russia      -7.313
Moscow           2000-02-01  Russia      -3.551
Moscow           2000-03-01  Russia      -1.661
Moscow           2000-04-01  Russia      10.096
Moscow           2000-05-01  Russia      10.357
...                     ...     ...         ...
Saint Petersburg 2013-05-01  Russia      12.355
Saint Petersburg 2013-06-01  Russia      17.185
Saint Petersburg 2013-07-01  Russia      17.234
Saint Petersburg 2013-08-01  Russia      17.153
Saint Petersburg 2013-09-01  Russia         NaN

[330 rows x 3 columns]

```

>**Setting and retrieveing from multi level indexes**
```python
# Index temperatures by country & city
temperatures_ind = temperatures.set_index(["country","city"])

# List of tuples: Brazil, Rio De Janeiro & Pakistan, Lahore
rows_to_keep = [("Brazil","Rio De Janeiro"), ("Pakistan","Lahore")]

# Subset for rows to keep
print(temperatures_ind.loc[rows_to_keep])

>>Output
                              date  avg_temp_c
country  city                                 
Brazil   Rio De Janeiro 2000-01-01      25.974
         Rio De Janeiro 2000-02-01      26.699
         Rio De Janeiro 2000-03-01      26.270
         Rio De Janeiro 2000-04-01      25.750
         Rio De Janeiro 2000-05-01      24.356
...                            ...         ...
Pakistan Lahore         2013-05-01      33.457
         Lahore         2013-06-01      34.456
         Lahore         2013-07-01      33.279
         Lahore         2013-08-01      31.511
         Lahore         2013-09-01         NaN

[330 rows x 2 columns]
```
>**We can pass a single list of outer indexes but we cant pass the single list of inner indexes**

>**Exapmle - Sorting by index values**
```python
# Sort temperatures_ind by index values
print(temperatures_ind.sort_index())

>>Output
                         date  avg_temp_c
country     city                         
Afghanistan Kabul  2000-01-01       3.326
            Kabul  2000-02-01       3.454
            Kabul  2000-03-01       9.612
            Kabul  2000-04-01      17.925
            Kabul  2000-05-01      24.658
...                       ...         ...
Zimbabwe    Harare 2013-05-01      18.298
            Harare 2013-06-01      17.020
            Harare 2013-07-01      16.299
            Harare 2013-08-01      19.232
            Harare 2013-09-01         NaN

[16500 rows x 2 columns]

# Sort temperatures_ind by index values at the city level
print(temperatures_ind.sort_index(level="city"))

>>Output
                            date  avg_temp_c
country       city                          
Côte D'Ivoire Abidjan 2000-01-01      27.293
              Abidjan 2000-02-01      27.685
              Abidjan 2000-03-01      29.061
              Abidjan 2000-04-01      28.162
              Abidjan 2000-05-01      27.547
...                          ...         ...
China         Xian    2013-05-01      18.979
              Xian    2013-06-01      23.522
              Xian    2013-07-01      25.251
              Xian    2013-08-01      24.528
              Xian    2013-09-01         NaN

[16500 rows x 2 columns]

# Sort temperatures_ind by country then descending city
print(temperatures_ind.sort_index(level=["country", "city"], ascending = [True, False]))

>>Output
                         date  avg_temp_c
country     city                         
Afghanistan Kabul  2000-01-01       3.326
            Kabul  2000-02-01       3.454
            Kabul  2000-03-01       9.612
            Kabul  2000-04-01      17.925
            Kabul  2000-05-01      24.658
...                       ...         ...
Zimbabwe    Harare 2013-05-01      18.298
            Harare 2013-06-01      17.020
            Harare 2013-07-01      16.299
            Harare 2013-08-01      19.232
            Harare 2013-09-01         NaN

[16500 rows x 2 columns]
```

## Slicing and Subsetting by .loc and .iloc
>**Sort index before you slice**
```python
dogs_srt = dogs.set_index(["breed","color"]).sort_index()
```

>**Slicing the outer level index .... We specify index values with :**
```python
dogs_srt.loc["Chow Chow":"Poodle"] # Note that the final value "Poodle" is included
# It willr eturn all Breed names between "Chow Chow" and "Poodle" including the boundaries
```

>**Slicing the inner index levels with simple list doesnt work**
```python
dogs_srt.loc["Tan":"Gray"]
# This retuen empty dataFrame
```

>**Slicing with inner index levels**
```python
dogs_srt.loc[("Labrador","Brown"):("Schnauzer","Grey")]
#It will return all the records between ("Labrador","Brown") and ("Schnauzer","Grey")
```

>**Slicing Columns**
```python
dogs_srt.loc[:, "name":"height_cm"]
#Returns all the rows and columns between name and height_cm
```

>**Slicing Rows and columns at the same time**
```python
dogs_srt.loc[("Labrador","Brown"):("Schnauzer","Grey"), "name":"height_cm"]
```

>**Subsetting based on range of dates**
```python
dogs = dogs.set_index("date_of_birth").sort_index()
# Get dogs with date_of_birth between 2014-08-25 and 2016-09-16
dogs.loc["2014-08-25":"2016-09-16"]
```

>**Slicing based on partial dates**
```python
# Get dogs with date_of_birth between 2014-01-01 and 2016-12-31
dogs.loc["2014":"2016"] #We can just slice based on years
```

>**Subsetting by row/ column number using .iloc**
```python
print(dogs.iloc[2:5, 1:4]) 
#Slices 3rd through 5th row and 2nd through 4th column. i.e. the final values aren't included in the slice
```

>**Example1 - Slicing Index values**

> Slicing lets you select consecutive elements of an object using first:last syntax. DataFrames can be sliced by index values or by row/column number; we'll start with the first case. This involves slicing inside the .loc[] method.

**Compared to slicing lists, there are a few things to remember.**

- You can only slice an index if the index is sorted (using .sort_index()).
- To slice at the outer level, first and last can be strings.
- To slice at inner levels, first and last should be tuples.
- If you pass a single slice to .loc[], it will slice the rows.

```python
# Sort the index of temperatures_ind
temperatures_srt = temperatures_ind.sort_index()

# Subset rows from Pakistan to Russia
print(temperatures_srt.loc["Pakistan":"Russia"])

>>Output
                                date  avg_temp_c
country  city                                   
Pakistan Faisalabad       2000-01-01      12.792
         Faisalabad       2000-02-01      14.339
         Faisalabad       2000-03-01      20.309
         Faisalabad       2000-04-01      29.072
         Faisalabad       2000-05-01      34.845
...                              ...         ...
Russia   Saint Petersburg 2013-05-01      12.355
         Saint Petersburg 2013-06-01      17.185
         Saint Petersburg 2013-07-01      17.234
         Saint Petersburg 2013-08-01      17.153
         Saint Petersburg 2013-09-01         NaN

[1155 rows x 2 columns]

# Try to subset rows from Lahore to Moscow. It tries to return non sense
print(temperatures_srt.loc["Lahore":"Moscow"])

>> Output
                         date  avg_temp_c
country city                             
Mexico  Mexico     2000-01-01      12.694
        Mexico     2000-02-01      14.677
        Mexico     2000-03-01      17.376
        Mexico     2000-04-01      18.294
        Mexico     2000-05-01      18.562
...                       ...         ...
Morocco Casablanca 2013-05-01      19.217
        Casablanca 2013-06-01      23.649
        Casablanca 2013-07-01      27.488
        Casablanca 2013-08-01      27.952
        Casablanca 2013-09-01         NaN

[330 rows x 2 columns]

# Subset rows from Pakistan, Lahore to Russia, Moscow
print(temperatures_srt.loc[("Pakistan", "Lahore"):("Russia","Moscow")])

>> Output
                      date  avg_temp_c
country  city                         
Pakistan Lahore 2000-01-01      12.792
         Lahore 2000-02-01      14.339
         Lahore 2000-03-01      20.309
         Lahore 2000-04-01      29.072
         Lahore 2000-05-01      34.845
...                    ...         ...
Russia   Moscow 2013-05-01      16.152
         Moscow 2013-06-01      18.718
         Moscow 2013-07-01      18.136
         Moscow 2013-08-01      17.485
         Moscow 2013-09-01         NaN

[660 rows x 2 columns]
```
>**Slicing in Both directions**
>> You've seen slicing DataFrames by rows and by columns, but since DataFrames are two-dimensional objects, it is often natural to slice both dimensions at once. That is, by passing two arguments to .loc[], you can subset by rows and columns in one go.
```python
# Subset rows from India, Hyderabad to Iraq, Baghdad
print(temperatures_srt.loc[("India","Hyderabad") : ("Iraq","Baghdad")])

>> Output
                        date  avg_temp_c
country city                            
India   Hyderabad 2000-01-01      23.779
        Hyderabad 2000-02-01      25.826
        Hyderabad 2000-03-01      28.821
        Hyderabad 2000-04-01      32.698
        Hyderabad 2000-05-01      32.438
...                      ...         ...
Iraq    Baghdad   2013-05-01      28.673
        Baghdad   2013-06-01      33.803
        Baghdad   2013-07-01      36.392
        Baghdad   2013-08-01      35.463
        Baghdad   2013-09-01         NaN

[2145 rows x 2 columns]

# Subset columns from date to avg_temp_c
print(temperatures_srt.loc[:, "date":"avg_temp_c"])

>> Output

                         date  avg_temp_c
country     city                         
Afghanistan Kabul  2000-01-01       3.326
            Kabul  2000-02-01       3.454
            Kabul  2000-03-01       9.612
            Kabul  2000-04-01      17.925
            Kabul  2000-05-01      24.658
...                       ...         ...
Zimbabwe    Harare 2013-05-01      18.298
            Harare 2013-06-01      17.020
            Harare 2013-07-01      16.299
            Harare 2013-08-01      19.232
            Harare 2013-09-01         NaN

[16500 rows x 2 columns]

# Subset in both directions at once
print(temperatures_srt.loc[("India","Hyderabad"):("Iraq","Baghdad"), "date":"avg_temp_c"])

>> Output
                        date  avg_temp_c
country city                            
India   Hyderabad 2000-01-01      23.779
        Hyderabad 2000-02-01      25.826
        Hyderabad 2000-03-01      28.821
        Hyderabad 2000-04-01      32.698
        Hyderabad 2000-05-01      32.438
...                      ...         ...
Iraq    Baghdad   2013-05-01      28.673
        Baghdad   2013-06-01      33.803
        Baghdad   2013-07-01      36.392
        Baghdad   2013-08-01      35.463
        Baghdad   2013-09-01         NaN

[2145 rows x 2 columns]
```

>**Example - Slicing Time Series**
>> Slicing is particularly useful for time series since it's a common thing to want to filter for data within a date range. Add the date column to the index, then use .loc[] to perform the subsetting. The important thing to remember is to keep your dates in ISO 8601 format, that is, "yyyy-mm-dd" for year-month-day, "yyyy-mm" for year-month, and "yyyy" for year.

```Python
# Use Boolean conditions to subset temperatures for rows in 2010 and 2011
temperatures_bool = temperatures[(temperatures["date"] >= "2010-01-01") & (temperatures["date"] <= "2011-12-31")]
print(temperatures_bool)

>> Output
            date     city        country  avg_temp_c
120   2010-01-01  Abidjan  Côte D'Ivoire      28.270
121   2010-02-01  Abidjan  Côte D'Ivoire      29.262
122   2010-03-01  Abidjan  Côte D'Ivoire      29.596
123   2010-04-01  Abidjan  Côte D'Ivoire      29.068
124   2010-05-01  Abidjan  Côte D'Ivoire      28.258
...          ...      ...            ...         ...
16474 2011-08-01     Xian          China      23.069
16475 2011-09-01     Xian          China      16.775
16476 2011-10-01     Xian          China      12.587
16477 2011-11-01     Xian          China       7.543
16478 2011-12-01     Xian          China      -0.490

[2400 rows x 4 columns]

# Set date as the index and sort the index
temperatures_ind = temperatures.set_index("date").sort_index()

# Use .loc[] to subset temperatures_ind for rows in 2010 and 2011
print(temperatures_ind.loc["2010":"2011"])

>> Output
                  city    country  avg_temp_c
date                                         
2010-01-01  Faisalabad   Pakistan      11.810
2010-01-01   Melbourne  Australia      20.016
2010-01-01   Chongqing      China       7.921
2010-01-01   São Paulo     Brazil      23.738
2010-01-01   Guangzhou      China      14.136
...                ...        ...         ...
2011-12-01      Nagoya      Japan       6.476
2011-12-01   Hyderabad      India      23.613
2011-12-01        Cali   Colombia      21.559
2011-12-01        Lima       Peru      18.293
2011-12-01     Bangkok   Thailand      25.021

[2400 rows x 3 columns]

# Use .loc[] to subset temperatures_ind for rows from Aug 2010 to Feb 2011
print(temperatures_ind.loc["2010-08":"2011-02"])

>> Output
                city        country  avg_temp_c
date                                           
2010-08-01  Calcutta          India      30.226
2010-08-01      Pune          India      24.941
2010-08-01     Izmir         Turkey      28.352
2010-08-01   Tianjin          China      25.543
2010-08-01    Manila    Philippines      27.101
...              ...            ...         ...
2011-02-01     Kabul    Afghanistan       3.914
2011-02-01   Chicago  United States       0.276
2011-02-01    Aleppo          Syria       8.246
2011-02-01     Delhi          India      18.136
2011-02-01   Rangoon          Burma      26.631

[700 rows x 3 columns]
```

>**Example - Subsettting by row/column numner**
>> The most common ways to subset rows are the ways we've previously discussed: using a Boolean condition or by index labels. However, it is also occasionally useful to pass row numbers.

>> This is done using .iloc[], and like .loc[], it can take two arguments to let you subset by rows and columns.
```python
# Get 23rd row, 2nd column (index 22, 1)
print(temperatures.iloc[22, 1])

>> Output
Abidjan

# Use slicing to get the first 5 rows
print(temperatures.iloc[0:5])

>> Output

        date     city        country  avg_temp_c
0 2000-01-01  Abidjan  Côte D'Ivoire      27.293
1 2000-02-01  Abidjan  Côte D'Ivoire      27.685
2 2000-03-01  Abidjan  Côte D'Ivoire      29.061
3 2000-04-01  Abidjan  Côte D'Ivoire      28.162
4 2000-05-01  Abidjan  Côte D'Ivoire      27.547

# Use slicing to get columns 3 to 4
print(temperatures.iloc[:, 2:4])

>> Output
             country  avg_temp_c
0      Côte D'Ivoire      27.293
1      Côte D'Ivoire      27.685
2      Côte D'Ivoire      29.061
3      Côte D'Ivoire      28.162
4      Côte D'Ivoire      27.547
...              ...         ...
16495          China      18.979
16496          China      23.522
16497          China      25.251
16498          China      24.528
16499          China         NaN

[16500 rows x 2 columns]

# Use slicing in both directions at once
print(temperatures.iloc[0:5, 2:4])

>> Output
         country  avg_temp_c
0  Côte D'Ivoire      27.293
1  Côte D'Ivoire      27.685
2  Côte D'Ivoire      29.061
3  Côte D'Ivoire      28.162
4  Côte D'Ivoire      27.547

```
## Working with Pivot tables and slicing

```python
dogs_height_by_breed_vs_color = dog_pack.pivot_table("height_cm", index="breed", columns="color")
#Index columns are the rows and columns arguments are the columns
```
>**.loc() + Slicing is a powerful combo**
```
dogs_height_by_breed_vs_color.loc["Chow Chow":"Poodle"]
# It will treat pivot table as a dataframe
```

>**Axis argument**
>>**Summary stats across all columns**
```python
dogs_height_by_breed_vs_color.mean(axis="index")
#Calculates mean by rows
```
>>**Summary stats across all rows**
```python
dogs_height_by_breed_vs_color.mean(axis="columns")
#Calculates mean by All Columns
```

>**Example - Pivot temperature by city and year**
>> It's interesting to see how temperatures for each city change over time—looking at every month results in a big table, which can be tricky to reason about. Instead, let's look at how temperatures change by year.

>> You can access the components of a date (year, month and day) using code of the form dataframe["column"].dt.component. For example, the month component is dataframe["column"].dt.month, and the year component is dataframe["column"].dt.year.

>>Once you have the year column, you can create a pivot table with the data aggregated by city and year, which you'll explore in the coming exercises.

```python
# Add a year column to temperatures
temperatures["year"] = temperatures["date"].dt.year

# Pivot avg_temp_c by country and city vs year
temp_by_country_city_vs_year = temperatures.pivot_table("avg_temp_c", index=["country", "city"], columns="year")

# See the result
print(temp_by_country_city_vs_year)

>> Output

year                              2000    2001    2002    2003    2004  ...    2009    2010    2011    2012    2013
country       city                                                      ...                                        
Afghanistan   Kabul             15.823  15.848  15.715  15.133  16.128  ...  15.093  15.676  15.812  14.510  16.206
Angola        Luanda            24.410  24.427  24.791  24.867  24.216  ...  24.325  24.440  24.151  24.240  24.554
Australia     Melbourne         14.320  14.180  14.076  13.986  13.742  ...  14.647  14.232  14.191  14.269  14.742
              Sydney            17.567  17.854  17.734  17.592  17.870  ...  18.176  17.999  17.713  17.474  18.090
Bangladesh    Dhaka             25.905  25.931  26.095  25.927  26.136  ...  26.536  26.648  25.803  26.284  26.587
...                                ...     ...     ...     ...     ...  ...     ...     ...     ...     ...     ...
United States Chicago           11.090  11.703  11.532  10.482  10.943  ...  10.298  11.816  11.214  12.821  11.587
              Los Angeles       16.643  16.466  16.430  16.945  16.553  ...  16.677  15.887  15.875  17.090  18.121
              New York           9.969  10.931  11.252   9.836  10.389  ...  10.142  11.358  11.272  11.971  12.164
Vietnam       Ho Chi Minh City  27.589  27.832  28.065  27.828  27.687  ...  27.853  28.282  27.675  28.249  28.455
Zimbabwe      Harare            20.284  20.861  21.079  20.889  20.308  ...  20.524  21.166  20.782  20.523  19.756

[100 rows x 14 columns]

```
>**Subsetting pivot tables**
>> A pivot table is just a DataFrame with sorted indexes, so the techniques you have learned already can be used to subset them. In particular, the .loc[] + slicing combination is often helpful.

```python
# Subset for Egypt to India
print(temp_by_country_city_vs_year.loc["Egypt":"India"])

>> Output
year                    2000    2001    2002    2003    2004  ...    2009    2010    2011    2012    2013
country  city                                                 ...                                        
Egypt    Alexandria   20.744  21.455  21.456  21.221  21.064  ...  21.671  22.460  21.181  21.553  21.439
         Cairo        21.486  22.331  22.414  22.171  22.082  ...  22.625  23.718  21.987  22.484  22.907
         Gizeh        21.486  22.331  22.414  22.171  22.082  ...  22.625  23.718  21.987  22.484  22.907
Ethiopia Addis Abeba  18.241  18.296  18.470  18.321  18.293  ...  18.765  18.298  18.607  18.449  19.539
France   Paris        11.740  11.371  11.871  11.909  11.339  ...  11.464  10.410  12.326  11.220  11.012
Germany  Berlin       10.964   9.690  10.264  10.066   9.823  ...  10.062   8.607  10.556   9.964  10.121
India    Ahmadabad    27.436  27.198  27.719  27.404  27.628  ...  28.096  28.018  27.290  27.027  27.609
         Bangalore    25.338  25.528  25.755  25.925  25.252  ...  25.726  25.705  25.362  26.042  26.611
         Bombay       27.204  27.244  27.629  27.578  27.319  ...  27.845  27.765  27.385  27.193  26.713
         Calcutta     26.491  26.515  26.704  26.561  26.634  ...  27.153  27.289  26.407  26.935  27.369
         Delhi        26.048  25.863  26.634  25.721  26.240  ...  26.554  26.520  25.630  25.889  26.709
         Hyderabad    27.232  27.555  27.665  27.845  27.229  ...  28.027  27.693  27.409  28.019  28.851
         Jaipur       26.430  26.023  27.032  26.027  26.642  ...  26.919  26.818  25.916  25.885  26.844
         Kanpur       25.354  25.326  26.117  25.409  25.587  ...  25.987  26.022  25.062  25.445  26.121
         Lakhnau      25.354  25.326  26.117  25.409  25.587  ...  25.987  26.022  25.062  25.445  26.121
         Madras       28.812  29.163  29.246  29.273  28.811  ...  29.417  29.047  29.063  29.778  30.412
         Nagpur       26.181  26.322  26.753  26.504  26.406  ...  27.139  26.927  26.005  26.328  27.112
         New Delhi    26.048  25.863  26.634  25.721  26.240  ...  26.554  26.520  25.630  25.889  26.709
         Pune         25.111  25.338  25.583  25.748  25.316  ...  25.868  25.749  25.161  25.297  25.848
         Surat        27.029  26.897  27.348  27.231  27.291  ...  27.820  27.682  27.017  26.889  27.438

[20 rows x 14 columns]

# Subset for Egypt, Cairo to India, Delhi
print(temp_by_country_city_vs_year.loc[("Egypt", "Cairo"): ("India","Delhi")])

>> Output
year                    2000    2001    2002    2003    2004  ...    2009    2010    2011    2012    2013
country  city                                                 ...                                        
Egypt    Cairo        21.486  22.331  22.414  22.171  22.082  ...  22.625  23.718  21.987  22.484  22.907
         Gizeh        21.486  22.331  22.414  22.171  22.082  ...  22.625  23.718  21.987  22.484  22.907
Ethiopia Addis Abeba  18.241  18.296  18.470  18.321  18.293  ...  18.765  18.298  18.607  18.449  19.539
France   Paris        11.740  11.371  11.871  11.909  11.339  ...  11.464  10.410  12.326  11.220  11.012
Germany  Berlin       10.964   9.690  10.264  10.066   9.823  ...  10.062   8.607  10.556   9.964  10.121
India    Ahmadabad    27.436  27.198  27.719  27.404  27.628  ...  28.096  28.018  27.290  27.027  27.609
         Bangalore    25.338  25.528  25.755  25.925  25.252  ...  25.726  25.705  25.362  26.042  26.611
         Bombay       27.204  27.244  27.629  27.578  27.319  ...  27.845  27.765  27.385  27.193  26.713
         Calcutta     26.491  26.515  26.704  26.561  26.634  ...  27.153  27.289  26.407  26.935  27.369
         Delhi        26.048  25.863  26.634  25.721  26.240  ...  26.554  26.520  25.630  25.889  26.709

[10 rows x 14 columns]

# Subset for Egypt, Cairo to India, Delhi, and 2005 to 2010
print(temp_by_country_city_vs_year.loc[("Egypt","Cairo"):("India","Delhi"), "2005":"2010"])

>> Output

year                    2005    2006    2007    2008    2009    2010
country  city                                                       
Egypt    Cairo        22.006  22.050  22.361  22.644  22.625  23.718
         Gizeh        22.006  22.050  22.361  22.644  22.625  23.718
Ethiopia Addis Abeba  18.313  18.427  18.143  18.165  18.765  18.298
France   Paris        11.553  11.788  11.751  11.278  11.464  10.410
Germany  Berlin        9.919  10.545  10.883  10.658  10.062   8.607
India    Ahmadabad    26.828  27.283  27.511  27.049  28.096  28.018
         Bangalore    25.477  25.418  25.464  25.353  25.726  25.705
         Bombay       27.036  27.382  27.635  27.178  27.845  27.765
         Calcutta     26.729  26.986  26.585  26.522  27.153  27.289
         Delhi        25.716  26.366  26.146  25.675  26.554  26.520

```

>**Example - Calculating on a pivot table**
>> Pivot tables are filled with summary statistics, but they are only a first step to finding something insightful. Often you'll need to perform further calculations on them. A common thing to do is to find the rows or columns where the highest or lowest value occurs.
>> Recall from Chapter 1 that you can easily subset a Series or DataFrame to find rows of interest using a logical condition inside of square brackets. For example: series[series > value].
```python
# Get the worldwide mean temp by year
mean_temp_by_year = temp_by_country_city_vs_year.mean(axis="index")

# Filter for the year that had the highest mean temp
print(mean_temp_by_year[mean_temp_by_year == mean_temp_by_year.max()])

>> Output
year
2013    20.312
dtype: float64

# Get the mean temp by city
mean_temp_by_city = temp_by_country_city_vs_year.mean(axis="columns")

# Filter for the city that had the lowest mean temp
print(mean_temp_by_city[mean_temp_by_city == mean_temp_by_city.min()])

>> Output
country  city  
China    Harbin    4.877
dtype: float64

```
# Visualizing your data

## Histograms

```python
import matplotlib.pyplot as plt

dog_pack["height_cm"].hist()

plt.show()

# We can adjust bins based on specifying bins argument
dog_pack["height_cm"].hist(bins=20)

plt.show()
```

## Bar Plots. relationship between categorical variable and numerical variable

```python
avg_weight_by_breed = dog_pack.groupby("breed")["weight_kg"].mean()

#for plotting bar plots

avg_weight_by_breed.plot(kind="bar", title="Mean weight by Dog Breed")
plt.show()
```

## Line Plots is used to visualize changes in numerical variables over time

sully.plot(x="date", y="weight_kg", kind="line")
plt.show()

>**Rotating Axis labels**
```python
sully.plot(x="date", y="weight_kg", kind="line", rot=45) #Rotating x axis labels for better readability
plt.show()
```

## Scatter plots - great for visualizing relationship between two numerical variables
```python
dog_pack.plot(x="height_cm", y="weight_kg", kind="scatter")
plt.show()
```
>**Layering plots**
```python
dog_pack[dog_pack["sex"] == "M"]["height_cm"].hist()
dog_pack[dog_pack["sex"] == "F"]["height_cm"].hist()
plt.legend(["F", "M"]) #For displaying legend
plt.show()
```
>>**Adding Transparanc**
```python
dog_pack[dog_pack["sex"] == "M"]["height_cm"].hist(alpha=0.7) #0 means completely transparent i.e. invisible. 1 means opaque
dog_pack[dog_pack["sex"] == "F"]["height_cm"].hist(alpha=0.7)
plt.legend(["F", "M"]) #For displaying legend
plt.show()
```
>**Example - Which avocado size is most popular **
>> Avocados are increasingly popular and delicious in guacamole and on toast. The Hass Avocado Board keeps track of avocado supply and demand across the USA, including the sales of three different sizes of avocado. In this exercise, you'll use a bar plot to figure out which size is the most popular.

>> Bar plots are great for revealing relationships between categorical (size) and numeric (number sold) variables, but you'll often have to manipulate your data first in order to get the numbers you need for plotting.

```python
# Import matplotlib.pyplot with alias plt
import matplotlib.pyplot as plt

# Look at the first few rows of data
print(avocados.head())

# Get the total number of avocados sold of each size
nb_sold_by_size = avocados.groupby("size")["nb_sold"].sum()
print(nb_sold_by_size)
# Create a bar plot of the number of avocados sold by size
nb_sold_by_size.plot(kind="bar", rot=30)

# Show the plot
plt.show()
```
![alt text](https://github.com/heramb-joshi/DataCampDataAnalyst/blob/main/Images/BarPlot_Ex3.4.1.png)

>**Example - Changes in sales over time**
>> Line plots are designed to visualize the relationship between two numeric variables, where each data values is connected to the next one. They are especially useful for visualizing the change in a number over time since each time point is naturally connected to the next time point. In this exercise, you'll visualize the change in avocado sales over three years.
```python
# Import matplotlib.pyplot with alias plt
import matplotlib.pyplot as plt
avocados
# Get the total number of avocados sold on each date
nb_sold_by_date = avocados.groupby("date")["nb_sold"].sum()

# Create a line plot of the number of avocados sold by date
nb_sold_by_date.plot(x="date", y="nb_sold", kind="line")

# Show the plot
plt.show()
```
![alt text](https://github.com/heramb-joshi/DataCampDataAnalyst/blob/main/Images/LinePlot_Ex3.4.2.png)

>**Example - Avocado supply and demand**
>> Scatter plots are ideal for visualizing relationships between numerical variables. In this exercise, you'll compare the number of avocados sold to average price and see if they're at all related. If they're related, you may be able to use one number to predict the other.
```python
# Scatter plot of avg_price vs. nb_sold with title
avocados.plot(x="nb_sold", y="avg_price", kind="scatter", title="Number of avocados sold vs. average price")

# Show the plot
plt.show()
```
![alt text](https://github.com/heramb-joshi/DataCampDataAnalyst/blob/main/Images/ScatterPlot_Ex3.4.3.png)

>**Example - Price of conventional vs. organic avocados**
>> Creating multiple plots for different subsets of data allows you to compare groups. In this exercise, you'll create multiple histograms to compare the prices of conventional and organic avocados.
```python
# Modify histogram transparency to 0.5 
avocados[avocados["type"] == "conventional"]["avg_price"].hist()

# Modify histogram transparency to 0.5
avocados[avocados["type"] == "organic"]["avg_price"].hist()

# Add a legend
plt.legend(["conventional", "organic"])

# Show the plot
plt.show()
```
![alt text](https://github.com/heramb-joshi/DataCampDataAnalyst/blob/main/Images/Histogram_Ex3.4.4.png) 

```python
# Modify histogram transparency to 0.5 
avocados[avocados["type"] == "conventional"]["avg_price"].hist(alpha=0.5)

# Modify histogram transparency to 0.5
avocados[avocados["type"] == "organic"]["avg_price"].hist(alpha=0.5)

# Add a legend
plt.legend(["conventional", "organic"])

# Show the plot
plt.show()
```
![alt text](https://github.com/heramb-joshi/DataCampDataAnalyst/blob/main/Images/Histogram_Ex3.4.5.png)

```python
# Modify bins to 20
avocados[avocados["type"] == "conventional"]["avg_price"].hist(alpha=0.5, bins=20)

# Modify bins to 20
avocados[avocados["type"] == "organic"]["avg_price"].hist(alpha=0.5, bins=20)

# Add a legend
plt.legend(["conventional", "organic"])

# Show the plot
plt.show()
```
![alt text](https://github.com/heramb-joshi/DataCampDataAnalyst/blob/main/Images/Histogram_Ex3.4.6.png)


## Missing Values
>**Missing values are indicated by NaN**
>**Detecting Missing Values**
```python
dogs.isna() #We get bollean values for every single value wherere there are any nulls
```

>**Detecting any Missing Values**
```python
dogs.isna().any() #We get 1 value for each variable
```

>**Counting Missing Values**
```python
dogs.isna().sum() #Count number of missing values
```

>**Plotting missing values**
```python
import matplot.pyplot as plt
dogs.isna().sum().plot(kind="bar")
plt.show()
```

>**Removing missing values**
```python
dogs.dropna() #removed rows with any mssing values
```

>**Replacing mssing values**
```python
dogs.fillna(0) #replaces all NaNs with 0
```
>**Example - Finding Missing values**
>> Missing values are everywhere, and you don't want them interfering with your work. Some functions ignore missing data by default, but that's not always the behavior you might want. Some functions can't handle missing values at all, so these values need to be taken care of before you can use them. If you don't know where your missing values are, or if they exist, you could make mistakes in your analysis. In this exercise, you'll determine if there are missing values in the dataset, and if so, how many.
```python
# Import matplotlib.pyplot with alias plt
import matplotlib.pyplot as plt

# Check individual values for missing values
print(avocados_2016.isna())

>>Output
     date  avg_price  total_sold  small_sold  large_sold  xl_sold  total_bags_sold  small_bags_sold  large_bags_sold  xl_bags_sold
0   False      False       False       False       False    False            False            False            False         False
1   False      False       False       False       False    False            False            False            False         False
2   False      False       False       False        True    False            False            False            False         False
3   False      False       False       False       False    False            False            False            False         False
4   False      False       False       False       False     True            False            False            False         False
5   False      False       False        True       False    False            False            False            False         False
6   False      False       False       False       False    False            False            False            False         False
7   False      False       False       False        True    False            False            False            False         False
8   False      False       False       False       False    False            False            False            False         False
9   False      False       False       False       False    False            False            False            False         False
10  False      False       False       False        True    False            False            False            False         False
11  False      False       False       False       False    False            False            False            False         False
12  False      False       False       False       False    False            False            False            False         False
13  False      False       False       False       False    False            False            False            False         False
14  False      False       False       False       False    False            False            False            False         False
15  False      False       False       False        True    False            False            False            False         False
16  False      False       False       False       False     True            False            False            False         False
17  False      False       False       False       False    False            False            False            False         False
18  False      False       False       False       False    False            False            False            False         False
19  False      False       False       False        True    False            False            False            False         False
20  False      False       False       False       False    False            False            False            False         False
21  False      False       False       False       False    False            False            False            False         False
22  False      False       False       False       False    False            False            False            False         False
23  False      False       False       False       False    False            False            False            False         False
24  False      False       False       False       False    False            False            False            False         False
25  False      False       False       False       False    False            False            False            False         False
26  False      False       False       False       False    False            False            False            False         False
27  False      False       False       False       False    False            False            False            False         False
28  False      False       False       False       False    False            False            False            False         False
29  False      False       False       False       False    False            False            False            False         False
30  False      False       False       False       False     True            False            False            False         False
31  False      False       False       False       False    False            False            False            False         False
32  False      False       False       False       False     True            False            False            False         False
33  False      False       False       False       False    False            False            False            False         False
34  False      False       False       False       False    False            False            False            False         False
35  False      False       False       False       False    False            False            False            False         False
36  False      False       False        True       False    False            False            False            False         False
37  False      False       False       False        True    False            False            False            False         False
38  False      False       False       False       False    False            False            False            False         False
39  False      False       False       False       False    False            False            False            False         False
40  False      False       False        True       False    False            False            False            False         False
41  False      False       False       False       False    False            False            False            False         False
42  False      False       False       False       False    False            False            False            False         False
43  False      False       False       False       False    False            False            False            False         False
44  False      False       False        True       False    False            False            False            False         False
45  False      False       False       False       False    False            False            False            False         False
46  False      False       False       False       False    False            False            False            False         False
47  False      False       False       False       False    False            False            False            False         False
48  False      False       False       False       False    False            False            False            False         False
49  False      False       False       False       False    False            False            False            False         False
50  False      False       False        True       False    False            False            False            False         False
51  False      False       False        True       False    False            False            False            False         False

# Check each column for missing values
print(avocados_2016.isna().any())

>> Output
date               False
avg_price          False
total_sold         False
small_sold          True
large_sold          True
xl_sold             True
total_bags_sold    False
small_bags_sold    False
large_bags_sold    False
xl_bags_sold       False
dtype: bool

# Bar plot of missing values by variable
avocados_2016.isna().sum().plot(kind="bar")

# Show plot
plt.show()
```
![alt text](https://github.com/heramb-joshi/DataCampDataAnalyst/blob/main/Images/BarPlot_Ex3.4.7.png)


>**Exapmle - Removing missing values**
>> Now that you know there are some missing values in your DataFrame, you have a few options to deal with them. One way is to remove them from the dataset completely. In this exercise, you'll remove missing values by removing all rows that contain missing values.
```python
# Remove rows with missing values
avocados_complete = avocados_2016.dropna()

# Check if any columns contain missing values
print(avocados_complete.isna().sum())
print(avocados_complete.isna().any())

>> Output
date               0
avg_price          0
total_sold         0
small_sold         0
large_sold         0
xl_sold            0
total_bags_sold    0
small_bags_sold    0
large_bags_sold    0
xl_bags_sold       0
dtype: int64

<script.py> output:
    date               False
    avg_price          False
    total_sold         False
    small_sold         False
    large_sold         False
    xl_sold            False
    total_bags_sold    False
    small_bags_sold    False
    large_bags_sold    False
    xl_bags_sold       False
    dtype: bool
```

>**Exapmle - Replacing Missing values**
>> Another way of handling missing values is to replace them all with the same value. For numerical variables, one option is to replace values with 0— you'll do this here. However, when you replace missing values, you make assumptions about what a missing value means. In this case, you will assume that a missing number sold means that no sales for that avocado type were made that week.

>> In this exercise, you'll see how replacing missing values can affect the distribution of a variable using histograms. You can plot histograms for multiple variables at a time as follows:

>> dogs[["height_cm", "weight_kg"]].hist()

```python
# List the columns with missing values
cols_with_missing = ["small_sold", "large_sold", "xl_sold"]

# Create histograms showing the distributions cols_with_missing
avocados_2016[cols_with_missing].hist()

# Show the plot
plt.show()
```
![alt text](https://github.com/heramb-joshi/DataCampDataAnalyst/blob/main/Images/Histogram_Ex3.4.8.png)

```python
# Fill in missing values with 0
avocados_filled = avocados_2016.fillna(0)

# Create histograms of the filled columns
avocados_filled[cols_with_missing].hist()

# Show the plot
plt.show()
```
![alt text](https://github.com/heramb-joshi/DataCampDataAnalyst/blob/main/Images/Histogram_Ex3.4.9.png)

## Creating Data Frames
- From a list of dictionaries
  - Constructed row by row
  ![alt text](https://github.com/heramb-joshi/DataCampDataAnalyst/blob/main/Images/ListOfDictionaries_3.4.1.png)
  
  - Once list of dictionaries is available, we can create dataframe as
  ``` python
  new_dogs = pd.DataFrame(list_of_dictionaries)
  ```
  ![alt text](https://github.com/heramb-joshi/DataCampDataAnalyst/blob/main/Images/ListOfDictionaries_3.4.2.png)
  
- From a dictionary of lists
  - Constructed Column by column
  - Key = Column name
  - value = lis of column values
![alt text](https://github.com/heramb-joshi/DataCampDataAnalyst/blob/main/Images/DictionaryOfLists_3.4.1.png)
