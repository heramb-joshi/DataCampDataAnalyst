# Inner Join

## Inner join - will only returns rows having matching values on both tables
>**Merge syntax**
```python
ward_census = wards.merge(census, on='ward') #Two dataframes are merged on ward column
```

>**Suffixes syntax**
```python
ward_census = wards.merge(census, on='ward', suffixes=('_ward','_cen')) 
#Suffixes (overlapping columns) can be controlled instead of default _x and _y
```

>**Example - Your first inner join**
>> You have been tasked with figuring out what the most popular types of fuel used in Chicago taxis are. To complete the analysis, you need to merge the taxi_owners and taxi_veh tables together on the vid column. You can then use the merged table along with the .value_counts() method to find the most common fuel_type.

```python
# Merge the taxi_owners and taxi_veh tables
taxi_own_veh = taxi_owners.merge(taxi_veh, on="vid")

# Print the column names of the taxi_own_veh
print(taxi_own_veh.columns)

>> Output
Index(['rid', 'vid', 'owner_x', 'address', 'zip', 'make', 'model', 'year', 'fuel_type', 'owner_y'], dtype='object')

# Merge the taxi_owners and taxi_veh tables setting a suffix
taxi_own_veh = taxi_owners.merge(taxi_veh, on='vid', suffixes = ('_own','_veh'))

# Print the column names of taxi_own_veh
print(taxi_own_veh.columns)

>> Output
Index(['rid', 'vid', 'owner_own', 'address', 'zip', 'make', 'model', 'year', 'fuel_type', 'owner_veh'], dtype='object')

# Print the value_counts to find the most popular fuel_type
print(taxi_own_veh['fuel_type'].value_counts()) # It counts the distinct values and their counts

>> Output

HYBRID                    2792
GASOLINE                   611
FLEX FUEL                   89
COMPRESSED NATURAL GAS      27
Name: fuel_type, dtype: int64
```

>**Example -Inner joins and number of rows returned**
>> All of the merges you have studied to this point are called inner joins. It is necessary to understand that inner joins only return the rows with matching values in both tables. You will explore this further by reviewing the merge between the wards and census tables, then comparing it to merges of copies of these tables that are slightly altered, named wards_altered, and census_altered. The first row of the wards column has been changed in the altered tables. You will examine how this affects the merge between them. The tables have been loaded for you.

```python
# Merge the wards and census tables on the ward column
wards_census = wards.merge(census, on="ward")

# Print the shape of wards_census
print('wards_census table shape:', wards_census.shape)

>> Output
wards_census table shape: (50, 9)

# Print the first few rows of the wards_altered table to view the change 
print(wards_altered[['ward']].head())

# Merge the wards_altered and census tables on the ward column
wards_altered_census = wards_altered.merge(census, on="ward")

# Print the shape of wards_altered_census
print('wards_altered_census table shape:', wards_altered_census.shape)

>> Output
  ward
0   61
1    2
2    3
3    4
4    5
wards_altered_census table shape: (49, 9)

# Print the first few rows of the census_altered table to view the change 
print(census_altered[['ward']].head())

# Merge the wards and census_altered tables on the ward column
wards_census_altered = wards.merge(census_altered, on="ward")

# Print the shape of wards_census_altered
print('wards_census_altered table shape:', wards_census_altered.shape)

>> Output
   ward
0  None
1     2
2     3
3     4
4     5
wards_census_altered table shape: (49, 9)
```

## One-to-many relationships

>**One-to-one** = Every row in the left table is related to one and only one row in the right table
>**One-to-many** = Every row in the left table is related to one or more one rows in the right table
>**Example - One-to-many merge**
>> A business may have one or multiple owners. In this exercise, you will continue to gain experience with one-to-many merges by merging a table of business owners, called biz_owners, to the licenses table. Recall from the video lesson, with a one-to-many relationship, a row in the left table may be repeated if it is related to multiple rows in the right table. In this lesson, you will explore this further by finding out what is the most common business owner title. (i.e., secretary, CEO, or vice president)

```python
# Merge the licenses and biz_owners table on account
licenses_owners = licenses.merge(biz_owners, on="account")

# Group the results by title then count the number of accounts
counted_df = licenses_owners.groupby("title").agg({'account':'count'}) #This is the way to calculate multiple stats

# Sort the counted_df in desending order
sorted_df = counted_df.sort_values("account", ascending=False)

# Use .head() method to print the first few rows of sorted_df
print(sorted_df.head())

>> Output
                 account
title                   
PRESIDENT           6259
SECRETARY           5205
SOLE PROPRIETOR     1658
OTHER               1200
VICE PRESIDENT       970
```
## Merging Multiple DataFrames

>**Single Merge**
```python
grants.merge(licenses, on=['address', 'zip'])
```
>**Multiple Merge**
```python
grants_license_ward = grants.merge(licenses, on=['address', 'zip']) \ #Python will read this one line of code
                      .merge(wards, on='ward', suffixes==('_bus','_ward')
grants_license_ward.head()
```
>**Results**
```python
import matplot.pyplot as plt
grants_license_ward.groupby('ward')['grant'].agg('sum').plot(kind='bar', y='grant')
plt.show()
```
![alt text](https://github.com/heramb-joshi/DataCampDataAnalyst/blob/main/Images/Chapter%204/Merge_Multiple_4.4.1.png)

>**Three Tables Merge - Syntax**
```python

df1.merge(df2, on="col") \
   .merge(df3, on="col")
```

>**Four Tables Merge - Syntax**
```python

df1.merge(df2, on="col") \
   .merge(df3, on="col") \
   .merge(df4, on="col")
```
>**Example - Total riders in a month**
>> Your goal is to find the total number of rides provided to passengers passing through the Wilson station (station_name == 'Wilson') when riding Chicago's public transportation system on weekdays (day_type == 'Weekday') in July (month == 7). Luckily, Chicago provides this detailed data, but it is in three different tables. You will work on merging these tables together to answer the question. This data is different from the business related data you have seen so far, but all the information you need to answer the question is provided.

>> The cal, ridership, and stations DataFrames have been loaded for you. The relationship between the tables can be seen in the diagram below.

![alt text](https://github.com/heramb-joshi/DataCampDataAnalyst/blob/main/Images/Chapter%204/Diagram_4.4.1.png)
```python
# Merge the ridership and cal tables
ridership_cal = ridership.merge(cal, on=['year','month','day'])

# Merge the ridership, cal, and stations tables
ridership_cal_stations = ridership.merge(cal, on=['year','month','day']) \
            				.merge(stations, on="station_id")
                    
# Merge the ridership, cal, and stations tables
ridership_cal_stations = ridership.merge(cal, on=['year','month','day']) \
							.merge(stations, on='station_id')

# Create a filter to filter ridership_cal_stations
filter_criteria = ((ridership_cal_stations['month'] == 7) 
                   & (ridership_cal_stations['day_type'] == "Weekday") 
                   & (ridership_cal_stations['station_name'] == "Wilson"))

# Use .loc and the filter to select for rides
print(ridership_cal_stations.loc[filter_criteria, 'rides'].sum())

>> Output
140005
```
>**Example - 3 Three table merge**
>> To solidify the concept of a three DataFrame merge, practice another exercise. A reasonable extension of our review of Chicago business data would include looking at demographics information about the neighborhoods where the businesses are. A table with the median income by zip code has been provided to you. You will merge the licenses and wards tables with this new income-by-zip-code table called zip_demo.

```python
# Merge licenses and zip_demo, on zip; and merge the wards on ward
licenses_zip_ward = licenses.merge(zip_demo, on="zip") \
							.merge(wards, on="ward")
            			

# Print the results by alderman and show median income
print(licenses_zip_ward.groupby("alderman").agg({'income':'median'}))


>> Output
                             income
alderman                           
Ameya Pawar                 66246.0
Anthony A. Beale            38206.0
Anthony V. Napolitano       82226.0
Ariel E. Reyboras           41307.0
Brendan Reilly             110215.0
Brian Hopkins               87143.0
Carlos Ramirez-Rosa         66246.0
Carrie M. Austin            38206.0
Chris Taliaferro            55566.0
Daniel "Danny" Solis        41226.0
David H. Moore              33304.0
Deborah Mell                66246.0
Debra L. Silverstein        50554.0
Derrick G. Curtis           65770.0
Edward M. Burke             42335.0
Emma M. Mitts               36283.0
George Cardenas             33959.0
Gilbert Villegas            41307.0
Gregory I. Mitchell         24941.0
Harry Osterman              45442.0
Howard B. Brookins, Jr.     33304.0
James Cappleman             79565.0
Jason C. Ervin              41226.0
Joe Moore                   39163.0
John S. Arena               70122.0
Leslie A. Hairston          28024.0
Margaret Laurino            70122.0
Marty Quinn                 67045.0
Matthew J. O'Shea           59488.0
Michael R. Zalewski         42335.0
Michael Scott, Jr.          31445.0
Michelle A. Harris          32558.0
Michelle Smith             100116.0
Milagros "Milly" Santiago   41307.0
Nicholas Sposato            62223.0
Pat Dowell                  46340.0
Patrick Daley Thompson      41226.0
Patrick J. O'Connor         50554.0
Proco "Joe" Moreno          87143.0
Raymond A. Lopez            33959.0
Ricardo Munoz               31445.0
Roberto Maldonado           68223.0
Roderick T. Sawyer          32558.0
Scott Waguespack            68223.0
Susan Sadlowski Garza       38417.0
Tom Tunney                  88708.0
Toni L. Foulkes             27573.0
Walter Burnett, Jr.         87143.0
William D. Burns           107811.0
Willie B. Cochran           28024.0

```
>**One-to-many merge with multiple tables**
>> In this exercise, assume that you are looking to start a business in the city of Chicago. Your perfect idea is to start a company that uses goats to mow the lawn for other businesses. However, you have to choose a location in the city to put your goat farm. You need a location with a great deal of space and relatively few businesses and people around to avoid complaints about the smell. You will need to merge three tables to help you choose your location. The land_use table has info on the percentage of vacant land by city ward. The census table has population by ward, and the licenses table lists businesses by ward.
```python
# Merge land_use and census and merge result with licenses including suffixes
land_cen_lic = land_use.merge(census, on="ward") \
                       .merge(licenses, on="ward", suffixes=("_cen","_lic"))
                       
# Group by ward, pop_2010, and vacant, then count the # of accounts
pop_vac_lic = land_cen_lic.groupby(["ward","pop_2010","vacant"], 
                                   as_index=False).agg({'account':'count'}) 
                                   #As_index = False will make sure the result is available in the dataset format
                                   
# Sort pop_vac_lic and print the results
sorted_pop_vac_lic = pop_vac_lic.sort_values(["vacant","account","pop_2010"], 
                                             ascending=[False, True, True])

# Print the top few rows of sorted_pop_vac_lic
print(sorted_pop_vac_lic.head())

>> Output
   ward  pop_2010  vacant  account
47    7     51581      19       80
12   20     52372      15      123
1    10     51535      14      130
16   24     54909      13       98
7    16     51954      13      156

```
# Left Join
## Left Join
> Returns All rows from left table and only matching key columns from the right tables
> Merge with left join
```python
movie_taglines = movies.merge(taglines, on="id", how="left") 
#how defines how the join is perfromed. Default value is inner

#Whenever ther is no matching, missing data is reflected as NaN
```
>**Example - Left Join**
```python
# Merge movies and financials with a left join
movies_financials = movies.merge(financials, on="id", how="left")

# Count the number of rows in the budget column that are missing
number_of_missing_fin = movies_financials['budget'].isna().sum()

# Print the number of movies missing financials
print(number_of_missing_fin)

>> Output
1574
```
>**Example2 - Enriching a dataset**
>> Setting how='left' with the .merge()method is a useful technique for enriching or enhancing a dataset with additional information from a different table. In this exercise, you will start off with a sample of movie data from the movie series Toy Story. Your goal is to enrich this data by adding the marketing tag line for each movie. You will compare the results of a left join versus an inner join.
```python
# Merge the toy_story and taglines tables with a left join
toystory_tag = toy_story.merge(taglines, on="id", how="left")

# Print the rows and shape of toystory_tag
print(toystory_tag)
print(toystory_tag.shape)

>> Output
      id        title  popularity release_date                   tagline
0  10193  Toy Story 3      59.995   2010-06-16  No toy gets left behind.
1    863  Toy Story 2      73.575   1999-10-30        The toys are back!
2    862    Toy Story      73.640   1995-10-30                       NaN
(3, 5)

# Merge the toy_story and taglines tables with a inner join
toystory_tag = toy_story.merge(taglines, on="id")

# Print the rows and shape of toystory_tag
print(toystory_tag)
print(toystory_tag.shape)

>> Output
      id        title  popularity release_date                   tagline
0  10193  Toy Story 3      59.995   2010-06-16  No toy gets left behind.
1    863  Toy Story 2      73.575   1999-10-30        The toys are back!
(2, 5)
```

## Other Joins
> Right Join
>> Returns all of the rows from the right table and only those rows from the left table having matching key column values

>**Merge with right join - Syntax**
```python
tv_movies = movies.merge(tv_genre, how="right", left_on="id", right_on="movie_id") 
#left_on and right_on attributes are used to specify key columns from left and right tables respectively

```

> Outer Join
>> Returns all of the records from both tables regardless if there is a match between both the tables

>**Merge with outer join**
```python
family_comedy = family.merge(comedy, on="movie_id", how="outer", suffixes=("_fam", "_com"))
```

>**Example - Right join to find unique movies**
>> Most of the recent big-budget science fiction movies can also be classified as action movies. You are given a table of science fiction movies called scifi_movies and another table of action movies called action_movies. Your goal is to find which movies are considered only science fiction movies. Once you have this table, you can merge the movies table in to see the movie names. Since this exercise is related to science fiction movies, use a right join as your superhero power to solve this problem.
```python
# Merge action_movies to scifi_movies with right join
action_scifi = action_movies.merge(scifi_movies, on='movie_id', how='right',
                                   suffixes=("_act", "_sci"))

# Print the first few rows of action_scifi to see the structure
print(action_scifi.head())

>>Output
       movie_id genre_act        genre_sci
    0        11    Action  Science Fiction
    1        18    Action  Science Fiction
    2        19       NaN  Science Fiction
    3        38       NaN  Science Fiction
    4        62       NaN  Science Fiction

# From action_scifi, select only the rows where the genre_act column is null
scifi_only = action_scifi[action_scifi["genre_act"].isna()]

# Merge the movies and scifi_only tables with an inner join
movies_and_scifi_only = movies.merge(scifi_only, left_on = "id", right_on="movie_id")

# Print the first few rows and shape of movies_and_scifi_only
print(movies_and_scifi_only.head())
print(movies_and_scifi_only.shape)

>> Output

      id                         title  popularity release_date  movie_id genre_act        genre_sci
0  18841  The Lost Skeleton of Cadavra       1.681   2001-09-12     18841       NaN  Science Fiction
1  26672     The Thief and the Cobbler       2.439   1993-09-23     26672       NaN  Science Fiction
2  15301      Twilight Zone: The Movie      12.903   1983-06-24     15301       NaN  Science Fiction
3   8452                   The 6th Day      18.447   2000-11-17      8452       NaN  Science Fiction
4   1649    Bill & Ted's Bogus Journey      11.350   1991-07-19      1649       NaN  Science Fiction
(258, 7)

```
>**Example - Popular genres with right join**
>> What are the genres of the most popular movies? To answer this question, you need to merge data from the movies and movie_to_genres tables. In a table called pop_movies, the top 10 most popular movies in the movies table have been selected. To ensure that you are analyzing all of the popular movies, merge it with the movie_to_genres table using a right join. To complete your analysis, count the number of different genres. Also, the two tables can be merged by the movie ID. However, in pop_movies that column is called id, and in movies_to_genres it's called movie_id.

```python
# Use right join to merge the movie_to_genres and pop_movies tables
genres_movies = movie_to_genres.merge(pop_movies, how='right', 
                                      left_on="movie_id", 
                                      right_on = "id")
genres_movies.head()
# Count the number of genres
genre_count = genres_movies.groupby('genre').agg({'id':'count'})

# Plot a bar chart of the genre_count
genre_count.plot(kind='bar')
plt.show()
```
![alt text](https://github.com/heramb-joshi/DataCampDataAnalyst/blob/main/Images/Chapter%204/BarPlot_4.4.2.png)

>**Example3- Using outer join to select actors**
>> One cool aspect of using an outer join is that, because it returns all rows from both merged tables and null where they do not match, you can use it to find rows that do not have a match in the other table. To try for yourself, you have been given two tables with a list of actors from two popular movies: Iron Man 1 and Iron Man 2. Most of the actors played in both movies. Use an outer join to find actors who did not act in both movies.

>> The Iron Man 1 table is called iron_1_actors, and Iron Man 2 table is called iron_2_actors. Both tables have been loaded for you and a few rows printed so you can see the structure.

![alt text](https://github.com/heramb-joshi/DataCampDataAnalyst/blob/main/Images/Chapter%204/OuterJoin_4.4.2.png)

```python
# Merge iron_1_actors to iron_2_actors on id with outer join using suffixes
iron_1_and_2 = iron_1_actors.merge(iron_2_actors,
                                     on="id",
                                     how="outer",
                                     suffixes=("_1","_2"))

# Create an index that returns true if name_1 or name_2 are null
m = ((iron_1_and_2['name_1'].isna()) | 
     (iron_1_and_2['name_2'].isna()))

# Print the first few rows of iron_1_and_2
print(iron_1_and_2[m].head())

>> Output
                   character_1      id           name_1 character_2 name_2
0                       Yinsen   17857       Shaun Toub         NaN    NaN
2  Obadiah Stane / Iron Monger    1229     Jeff Bridges         NaN    NaN
3                  War Machine   18288  Terrence Howard         NaN    NaN
5                         Raza   57452      Faran Tahir         NaN    NaN
8                   Abu Bakaar  173810    Sayed Badreya         NaN    NaN
```

## Self Join
> **Merging date to itself with an inner join**
```python
original_sequels  = sequels.merge(sequels, left_on="sequel", right_on="id", suffixes=("_org","_seq"))
```
> **Merging date to itself with a left join**
```python
original_sequels  = sequels.merge(sequels, left_on="sequel", right_on="id", how="left", suffixes=("_org","_seq"))
# It will show all of the orginal movies and movies without sequels will have seq_ columns as NaN
```
> **When to use merge to itself**
>> Hierarchical relationships: Employee and manager
>> Sequential relationships: Logisitcs movement
>> Graph Data: Network of friends

> **Example - Self join**

>> Merging a table to itself can be useful when you want to compare values in a column to other values in the same column. In this exercise, you will practice this by creating a table that for each movie will list the movie director and a member of the crew on one row. You have been given a table called crews, which has columns id, job, and name. First, merge the table to itself using the movie ID. This merge will give you a larger table where for each movie, every job is matched against each other. Then select only those rows with a director in the left table, and avoid having a row where the director's job is listed in both the left and right tables. This filtering will remove job combinations that aren't with the director.

```python
# Merge the crews table to itself
crews_self_merged = crews.merge(crews, on='id', how='inner',
                                suffixes=('_dir','_crew'))

# Create a Boolean index to select the appropriate
boolean_filter = ((crews_self_merged['job_dir'] == "Director") & 
     (crews_self_merged['job_crew'] != "Director"))
direct_crews = crews_self_merged[boolean_filter]

# Print the first few rows of direct_crews
print(direct_crews.head())

>> Output 
        id   job_dir       name_dir        job_crew          name_crew
156  19995  Director  James Cameron          Editor  Stephen E. Rivkin
157  19995  Director  James Cameron  Sound Designer  Christopher Boyes
158  19995  Director  James Cameron         Casting          Mali Finn
160  19995  Director  James Cameron          Writer      James Cameron
161  19995  Director  James Cameron    Set Designer    Richard F. Mays
```

## Merging on indexes

> **Setting Index while importing**
```python
movies = pd.read_csv("tmdb_movies.csv", index_col=["id"])
```

> **Merging on an index**

```python
movies_taglines = movies.merge(taglines, on="id", how="left")
#In the output, id will be an index
```

> **Multi-index merge**
```python
samuel_casts = samuel.merge(casts, on=["movie_id","cast_id"])
```

> **Index merge with left_on and right_on - Inf index names are different in two tables**
```python
movies_genres = movies.merge(movie_to_genre, left_on="id", left_index=True, right_on="movie_id", right_index=True)
#To indicate that we are merging on indexes, we need to set these on true
```
> **Example 1 - Index merge for movie ratings**
>> To practice merging on indexes, you will merge movies and a table called ratings that holds info about movie ratings. Make sure your merge returns all of the rows from the movies table and not all the rows of ratings table need to be included in the result.

```python
# Merge to the movies table the ratings table on the index
movies_ratings = movies.merge(ratings, on="id", how="left")

# Print the first few rows of movies_ratings
print(movies_ratings.head())

>> Output
                      title  popularity release_date  vote_average  vote_count
id                                                                            
257            Oliver Twist      20.416   2005-09-23           6.7       274.0
14290  Better Luck Tomorrow       3.877   2002-01-12           6.5        27.0
38365             Grown Ups      38.864   2010-06-24           6.0      1705.0
9672               Infamous       3.681   2006-11-16           6.4        60.0
12819       Alpha and Omega      12.301   2010-09-17           5.3       124.0
```

> **Example - Do sequels earn more?**
>> It is time to put together many of the aspects that you have learned in this chapter. In this exercise, you'll find out which movie sequels earned the most compared to the original movie. To answer this question, you will merge a modified version of the sequels and financials tables where their index is the movie ID. You will need to choose a merge type that will return all of the rows from the sequels table and not all the rows of financials table need to be included in the result. From there, you will join the resulting table to itself so that you can compare the revenue values of the original movie to the sequel. Next, you will calculate the difference between the two revenues and sort the resulting dataset.

```python
# Merge sequels and financials on index id
sequels_fin = sequels.merge(financials, how="left", on="id")

print(sequels_fin.head())
              title sequel     budget    revenue
id                                              
19995        Avatar   <NA>  2.370e+08  2.788e+09
862       Toy Story    863  3.000e+07  3.736e+08
863     Toy Story 2  10193  9.000e+07  4.974e+08
597         Titanic   <NA>  2.000e+08  1.845e+09
24428  The Avengers   <NA>  2.200e+08  1.520e+09


# Self merge with suffixes as inner join with left on sequel and right on id
orig_seq = sequels_fin.merge(sequels_fin, how="inner", left_on="sequel", 
                             right_on="id", right_index=True,
                             suffixes=("_org","_seq"))

# Add calculation to subtract revenue_org from revenue_seq 
orig_seq['diff'] = orig_seq['revenue_seq'] - orig_seq['revenue_org']

# Select the title_org, title_seq, and diff 
titles_diff = orig_seq[["title_org","title_seq","diff"]]

# Print the first rows of the sorted titles_diff
print(titles_diff.sort_values("diff", ascending=False).head())

>> Output

                   title_org        title_seq       diff
    id                                                  
    331    Jurassic Park III   Jurassic World  1.145e+09
    272        Batman Begins  The Dark Knight  6.303e+08
    10138         Iron Man 2       Iron Man 3  5.915e+08
    863          Toy Story 2      Toy Story 3  5.696e+08
    10764  Quantum of Solace          Skyfall  5.225e+08
```

# Filtering joins

> **Mutating versus filtering joins**
- **Mutating Joins**
  - combines data from two tables based on matching observations in both tables
- **Filtering joins**
  - Filter observations from table based on whether or not they match an observation in another table

## Semi-joins
- Returns an interseation similar to inner join
- Returns columns from left tables only and **not** from the right table
- No duplicate rows are returned even if there is one to many relationships

![alt text](https://github.com/heramb-joshi/DataCampDataAnalyst/blob/main/Images/Chapter%204/SemiJoin_4.4.3.png)

> **Semi- join - Syntax. We want to find genres of top tracks**
>> **Step 1 -Inner join**

```python
genre_tracks = genres.merge(top_tracks, on="gid")
```

>> **Step2 - find the matches**

```python
genres["gid"].isin(genre_tracks["gid"])
```

>> **Step3 - Combine**

```python
top_genres = genres[genres["gid"].isin(genre_tracks["gid"])]

#This is called filtering join because we have filtered genre table based on whats available in top tracks tables
```
## Anti-joins
- Returns an observation in the left table that do not have matching observation in right table
- Only returns column from left table

![alt_text](https://github.com/heramb-joshi/DataCampDataAnalyst/blob/main/Images/Chapter%204/AntiJoin_4.4.3.png)

>> **Step 1 - Left join**
```python
genres_tracks = genres.merge(top_tracks, on="gid", how="left", indicator=True) 
#Adds column called _merge to output. to indicate source of the row whether the row exist in both tables or left_only
```

>> **Step 2 - using Loc[] find gids that are avilable only in left table**
```python
gid_list = genres_tracks.loc[genres_tracks["_merge"] == "left_only", "gid"]
```

>> **Step 3 - using .isin() to filters gids in genres tables**
```python
non_top_genres = genres[genres["gid"].isin(gid_lisy)]
```

> **Example - Performing an anti join**
>> In our music streaming company dataset, each customer is assigned an employee representative to assist them. In this exercise, filter the employee table by a table of top customers, returning only those employees who are not assigned to a customer. The results should resemble the results of an anti join. The company's leadership will assign these employees additional training so that they can work with high valued customers.

```python
# Merge employees and top_cust
empl_cust = employees.merge(top_cust, on="srid", 
                            how="left", indicator=True)
			    
# Select the srid column where _merge is left_only
srid_list = empl_cust.loc[empl_cust["_merge"] == "left_only", 'srid']

# Get employees not working with top customers
print(employees[employees["srid"].isin(srid_list)])

>> Output
   srid     lname    fname            title  hire_date                    email
0     1     Adams   Andrew  General Manager 2002-08-14   andrew@chinookcorp.com
1     2   Edwards    Nancy    Sales Manager 2002-05-01    nancy@chinookcorp.com
5     6  Mitchell  Michael       IT Manager 2003-10-17  michael@chinookcorp.com
6     7      King   Robert         IT Staff 2004-01-02   robert@chinookcorp.com
7     8  Callahan    Laura         IT Staff 2004-03-04    laura@chinookcorp.com
```

> **Example - Performing a semi join**
>> Some of the tracks that have generated the most significant amount of revenue are from TV-shows or are other non-musical audio. You have been given a table of invoices that include top revenue-generating items. Additionally, you have a table of non-musical tracks from the streaming service. In this exercise, you'll use a semi join to find the top revenue-generating non-musical tracks..

```python
# Merge the non_mus_tck and top_invoices tables on tid
tracks_invoices = non_mus_tcks.merge(top_invoices, on="tid")

# Use .isin() to subset non_mus_tcks to rows with tid in tracks_invoices
top_tracks = non_mus_tcks[non_mus_tcks['tid'].isin(tracks_invoices['tid'])]

# Group the top_tracks by gid and count the tid rows
cnt_by_gid = top_tracks.groupby(['gid'], as_index=False).agg({'tid':'count'})

# Merge the genres table to cnt_by_gid on gid and print
print(cnt_by_gid.merge(genres, on="gid"))

>> Output
   gid  tid      name
0   19    4  TV Shows
1   21    2     Drama
2   22    1    Comedy
```

## Concatenate dataframe vertically
- pandas .concat() method can concatenate both horizontally and vertically
  - axis = 0, vertical concatenation
  - Often data from different periods of time 

> **Basic concatenation**

```python
pd.concat([inv_jan, inv_feb, inv_mar]) # By default index= 0 so no need to specify it
#It will retain index values
```
>> **ignoring index**
```python
#If index doesnt contain valueable info we can ignore_index = True
pd.concat([inv_jan, inv_feb, inv_mar], ignore_index = True)
```

>> **Setting labels to original tables**

```python
pd.concat([inv_jan, inv_feb, inv_mar], ignore_index = False, keys=['jan','feb','mar']) 
#You cannot ignore index and set keys at the same time so ignore_index = False
#This will result in multi index with label as an outer index
```

> **Concatenate with different column names **

```python
pd.concat([inv_jan, inv_feb], sort=True) #Sorts columns alphabetically. Includes all the columns by default
```

>> **Include only matching columns**

```python
pd.concat([inv_jan, inv_feb], join="inner") #By default join="outer" so it includes all columns
```

> **Example - Concatenation basics**
>> You have been given a few tables of data with musical track info for different albums from the metal band, Metallica. The track info comes from their Ride The Lightning, Master Of Puppets, and St. Anger albums. Try various features of the .concat() method by concatenating the tables vertically together in different ways.

```python
# Concatenate the tracks
tracks_from_albums = pd.concat([tracks_master, tracks_ride, tracks_st],
                               sort=True)
print(tracks_from_albums)

>> Output
   aid             composer  gid  mtid                     name   tid  u_price
0  152  J.Hetfield/L.Ulrich    3     1                  Battery  1853     0.99
1  152            K.Hammett    3     1        Master Of Puppets  1854     0.99
4  152  J.Hetfield/L.Ulrich    3     1        Disposable Heroes  1857     0.99
0  154                  NaN    3     1     Fight Fire With Fire  1874     0.99
1  154                  NaN    3     1       Ride The Lightning  1875     0.99
2  154                  NaN    3     1  For Whom The Bell Tolls  1876     0.99
3  154                  NaN    3     1            Fade To Black  1877     0.99
4  154                  NaN    3     1        Trapped Under Ice  1878     0.99
0  155                  NaN    3     1                  Frantic  1882     0.99
1  155                  NaN    3     1                St. Anger  1883     0.99
2  155                  NaN    3     1     Some Kind Of Monster  1884     0.99
3  155                  NaN    3     1             Dirty Window  1885     0.99
4  155                  NaN    3     1            Invisible Kid  1886     0.99


# Concatenate the tracks so the index goes from 0 to n-1
tracks_from_albums = pd.concat([tracks_master, tracks_ride, tracks_st],
                               ignore_index=True,
                               sort=True)
print(tracks_from_albums)

>> Output
    aid             composer  gid  mtid                     name   tid  u_price
0   152  J.Hetfield/L.Ulrich    3     1                  Battery  1853     0.99
1   152            K.Hammett    3     1        Master Of Puppets  1854     0.99
2   152  J.Hetfield/L.Ulrich    3     1        Disposable Heroes  1857     0.99
3   154                  NaN    3     1     Fight Fire With Fire  1874     0.99
4   154                  NaN    3     1       Ride The Lightning  1875     0.99
5   154                  NaN    3     1  For Whom The Bell Tolls  1876     0.99
6   154                  NaN    3     1            Fade To Black  1877     0.99
7   154                  NaN    3     1        Trapped Under Ice  1878     0.99
8   155                  NaN    3     1                  Frantic  1882     0.99
9   155                  NaN    3     1                St. Anger  1883     0.99
10  155                  NaN    3     1     Some Kind Of Monster  1884     0.99
11  155                  NaN    3     1             Dirty Window  1885     0.99
12  155                  NaN    3     1            Invisible Kid  1886     0.99

# Concatenate the tracks, show only columns names that are in all tables
tracks_from_albums = pd.concat([tracks_master, tracks_ride, tracks_st],
                               join="inner",
                               sort=True)
print(tracks_from_albums)

>> Output

   aid  gid  mtid                     name   tid  u_price
0  152    3     1                  Battery  1853     0.99
1  152    3     1        Master Of Puppets  1854     0.99
4  152    3     1        Disposable Heroes  1857     0.99
0  154    3     1     Fight Fire With Fire  1874     0.99
1  154    3     1       Ride The Lightning  1875     0.99
2  154    3     1  For Whom The Bell Tolls  1876     0.99
3  154    3     1            Fade To Black  1877     0.99
4  154    3     1        Trapped Under Ice  1878     0.99
0  155    3     1                  Frantic  1882     0.99
1  155    3     1                St. Anger  1883     0.99
2  155    3     1     Some Kind Of Monster  1884     0.99
3  155    3     1             Dirty Window  1885     0.99
4  155    3     1            Invisible Kid  1886     0.99

```
> **Example - Concatenating with keys**
>> The leadership of the music streaming company has come to you and asked you for assistance in analyzing sales for a recent business quarter. They would like to know which month in the quarter saw the highest average invoice total. You have been given three tables with invoice data named inv_jul, inv_aug, and inv_sep. Concatenate these tables into one to create a graph of the average monthly invoice total.

```python
# Concatenate the tables and add keys
inv_jul_thr_sep = pd.concat([inv_jul, inv_aug, inv_sep], 
                            keys=['7Jul', '8Aug', '9Sep'])
inv_jul_thr_sep.head()
# Group the invoices by the index keys and find avg of the total column
avg_inv_by_month = inv_jul_thr_sep.groupby(level=0).agg({'total':'mean'}) #level= 0  groups by outer index

# Bar plot of avg_inv_by_month
avg_inv_by_month.plot(kind="bar")
plt.show()
```
![alt_text](https://github.com/heramb-joshi/DataCampDataAnalyst/blob/main/Images/Chapter%204/Bar_4.4.3.png)

## Verifying integrity

### Validating merges
> .merge(validate= None):
- Checks if merge is of specified type
- 'one_to_one'
- 'one_to_many'
- 'many_to_one'
- 'many_to_many'

> **merge validate: one_to_one**
```python
tracks.merge(specs, on="tid", validate='one_to_one')
# it gives error if one_to_one os not satisfied. MergeError:

>> Output
Traceback (most recent call last):
MergeError: Merge keys are not unique in right dataset; not a one-to-one merge

# For every album, there are mutlple tracks so validate one-to-many for albums to tracks will result in correct result
albums.merge(tracks, on="aid", validate="one_to_many")
```
### Validating concat methods
- It checks whether new concatenated index contains duplicates. It will check the index values and not the columns
- Syntax :- .concat(verify_integrity=False) #Default value is False

```python
pd.concat([inv_feb, inv_mar], verify_integrity=True)

>>Output
ValueError: Indexes have overlapping values

```
> **Example1- Concatenate and merge to find common songs**
>> The senior leadership of the streaming service is requesting your help again. You are given the historical files for a popular playlist in the classical music genre in 2018 and 2019. Additionally, you are given a similar set of files for the most popular pop music genre playlist on the streaming service in 2018 and 2019. Your goal is to concatenate the respective files to make a large classical playlist table and overall popular music table. Then filter the classical music table using a semi join to return only the most popular classical music tracks.

```python
# Concatenate the classic tables vertically
classic_18_19 = pd.concat([classic_18, classic_19], ignore_index=True)

# Concatenate the pop tables vertically
pop_18_19 = pd.concat([pop_18, pop_19], ignore_index=True)

# Merge classic_18_19 with pop_18_19
classic_pop = classic_18_19.merge(pop_18_19, on="tid")

# Using .isin(), filter classic_18_19 rows where tid is in classic_pop
popular_classic = classic_18_19[classic_18_19["tid"].isin(classic_pop["tid"])]

# Print popular chart
print(popular_classic)

>>Output
        pid   tid
    3    12  3479
    10   12  3439
    21   12  3445
    23   12  3449
    48   12  3437
    50   12  3435
```

# Using merge_ordered()
> This is used for merging time series and other ordered data
> Output is similar to traditional merge but this will sort output based on key columns

## Method comparison

![alt_text](https://github.com/heramb-joshi/DataCampDataAnalyst/blob/main/Images/Chapter%204/MergeMethodComparison_4.4.4.png)

> **Syntax**
```python
import pandas as pd
pd.merge_ordered(appl, mcd, on="date", suffixes=("_appl","_mcd"))
```
> We can fill in missing data using forward fill. It fills missing values with previous values
```
pd.merge_ordered(appl, mcd, on="date", suffixes=("_appl","_mcd"), fill_method="ffill")
```

## When to use merge_ordered()**
- When we are working on ordered/ time series data
- Filling in missing values using fill_method="ffill" i.e. fill forward method

>**Example1 - Correlation between GDP and S&P500**
>> In this exercise, you want to analyze stock returns from the S&P 500. You believe there may be a relationship between the returns of the S&P 500 and the GDP of the US. Merge the different datasets together to compute the correlation.

```python
# Use merge_ordered() to merge gdp and sp500 on year and date
gdp_sp500 = pd.merge_ordered(gdp, sp500, left_on=['year'], right_on=['date'], 
                             how='left')

# Print gdp_sp500
print(gdp_sp500)

>> Output
  country code  year        gdp    date  returns
0          USA  2010  1.499e+13  2010.0    12.78
1          USA  2011  1.554e+13  2011.0     0.00
2          USA  2012  1.620e+13  2012.0    13.41
3          USA  2012  1.620e+13  2012.0    13.41
4          USA  2013  1.678e+13  2013.0    29.60
5          USA  2014  1.752e+13  2014.0    11.39
6          USA  2015  1.822e+13  2015.0    -0.73
7          USA  2016  1.871e+13  2016.0     9.54
8          USA  2017  1.949e+13  2017.0    19.42
9          USA  2018  2.049e+13     NaN      NaN

# Use merge_ordered() to merge gdp and sp500, interpolate missing value
gdp_sp500 = pd.merge_ordered(gdp, sp500, left_on='year', right_on='date', 
                             how='left',  fill_method='ffill')

# Print gdp_sp500
print(gdp_sp500)

>> Output
      country code  year        gdp  date  returns
    0          USA  2010  1.499e+13  2010    12.78
    1          USA  2011  1.554e+13  2011     0.00
    2          USA  2012  1.620e+13  2012    13.41
    3          USA  2012  1.620e+13  2012    13.41
    4          USA  2013  1.678e+13  2013    29.60
    5          USA  2014  1.752e+13  2014    11.39
    6          USA  2015  1.822e+13  2015    -0.73
    7          USA  2016  1.871e+13  2016     9.54
    8          USA  2017  1.949e+13  2017    19.42
    9          USA  2018  2.049e+13  2017    19.42
    

# Subset the gdp and returns columns
gdp_returns = gdp_sp500[['gdp','returns']]

# Print gdp_returns correlation
print (gdp_returns.corr())

>> Output
           gdp  returns
gdp      1.000    0.212
returns  0.212    1.000
```

>**Example 2 - Phillips curve using merge_ordered().**

>> There is an economic theory developed by A. W. Phillips which states that inflation and unemployment have an inverse relationship. The theory claims that with economic growth comes inflation, which in turn should lead to more jobs and less unemployment.
>> 
>> You will take two tables of data from the U.S. Bureau of Labor Statistics, containing unemployment and inflation data over different periods, and create a Phillips curve. The tables have different frequencies. One table has a data entry every six months, while the other has a data entry every month. You will need to use the entries where you have data within both tables.

```python
# Use merge_ordered() to merge inflation, unemployment with inner join
inflation_unemploy = pd.merge_ordered(inflation, unemployment, on="date", how="inner")

# Print inflation_unemploy 
print(inflation_unemploy)

# Plot a scatter plot of unemployment_rate vs cpi of inflation_unemploy
inflation_unemploy.plot(x="unemployment_rate", y="cpi", kind="scatter")
plt.show()
```

![alt_text](https://github.com/heramb-joshi/DataCampDataAnalyst/blob/main/Images/Chapter%204/Scatter_4.4.4.png)

> **Example 2 - merge_ordered() caution, multiple columns**
>> When using merge_ordered() to merge on multiple columns, the order is important when you combine it with the forward fill feature. The function sorts the merge on columns in the order provided. In this exercise, we will merge GDP and population data from the World Bank for the Australia and Sweden, reversing the order of the merge on columns. The frequency of the series are different, the GDP values are quarterly, and the population is yearly. Use the forward fill feature to fill in the missing data. Depending on the order provided, the fill forward will use unintended data to fill in the missing values.

```python
# Merge gdp and pop on date and country with fill and notice rows 2 and 3
ctry_date = pd.merge_ordered(gdp, pop, on=["date", "country"],
                             fill_method='ffill')

# Print ctry_date
print(ctry_date)

>> Output
        date    country         gdp  series_code_x       pop series_code_y
0 1990-01-01  Australia  158051.132  NYGDPMKTPSAKD  17065100   SP.POP.TOTL
1 1990-01-01     Sweden   79837.846  NYGDPMKTPSAKD   8558835   SP.POP.TOTL
2 1990-04-01  Australia  158263.582  NYGDPMKTPSAKD   8558835   SP.POP.TOTL
3 1990-04-01     Sweden   80582.286  NYGDPMKTPSAKD   8558835   SP.POP.TOTL
4 1990-07-01  Australia  157329.279  NYGDPMKTPSAKD   8558835   SP.POP.TOTL

# Merge gdp and pop on country and date with fill
date_ctry = pd.merge_ordered(gdp, pop, on=["country","date"], fill_method="ffill")

# Print date_ctry
print(date_ctry)

>> Output
         date    country         gdp  series_code_x       pop series_code_y
0  1990-01-01  Australia  158051.132  NYGDPMKTPSAKD  17065100   SP.POP.TOTL
1  1990-04-01  Australia  158263.582  NYGDPMKTPSAKD  17065100   SP.POP.TOTL
2  1990-07-01  Australia  157329.279  NYGDPMKTPSAKD  17065100   SP.POP.TOTL
3  1990-09-01  Australia  158240.678  NYGDPMKTPSAKD  17065100   SP.POP.TOTL
4  1991-01-01  Australia  156195.954  NYGDPMKTPSAKD  17284000   SP.POP.TOTL
```

## Using merge_asof()
![alt_text](https://github.com/heramb-joshi/DataCampDataAnalyst/blob/main/Images/Chapter%204/merge_as_of_4.4.4.png)

> **Syntax**
```python
pd.merge_asof(visa, ibm, on="date_time", suffixes=("_visa","_ibm"))

#It will contain all rows of left table i.e. visa but it will only include close matches from ibm. lesser than on equal to the key values in visa
pd.merge_asof(visa, ibm, on="date_time", suffixes=("_visa","_ibm"), direction="forward") #Matches the next best value i.e. >=

#default value is backward. We can set the value to "nearest" which will find the best values based on forward or backward comparison

```

> **When to use merge_asof() method**
>> First, you might think of this method when you are working with data sampled from a process and the dates or times may not exactly align. This is similar to what we did in our example. 
>> It could also be used when you are working on a time-series training set, where you do not want any events from the future to be visible before that point in time. (direction="backward")

> **Example1 - Using merge_asof() to study stocks**
>> You have a feed of stock market prices that you record. You attempt to track the price every five minutes. Still, due to some network latency, the prices you record are roughly every 5 minutes. You pull your price logs for three banks, JP Morgan (JPM), Wells Fargo (WFC), and Bank Of America (BAC). You want to know how the price change of the two other banks compare to JP Morgan. Therefore, you will need to merge these three logs into one table. Afterward, you will use the pandas .diff() method to compute the price change over time. Finally, plot the price changes so you can review your analysis.

```python
# Use merge_asof() to merge jpm and wells
jpm_wells = pd.merge_asof(jpm, wells, on="date_time", suffixes=("", "_wells"), direction="nearest")


# Use merge_asof() to merge jpm_wells and bac
jpm_wells_bac = pd.merge_asof(jpm_wells, bac, on="date_time", suffixes=("_jpm", "_bac"),
                                direction="nearest")


# Compute price diff
price_diffs = jpm_wells_bac.diff()
price_diffs.head()

>>Output
        date_time  close_jpm  close_wells  close_bac
0             NaT        NaN          NaN        NaN
1 0 days 00:04:47      0.060       -0.003      0.000
2 0 days 00:04:57     -0.449       -0.130     -0.164
3 0 days 00:05:54      0.009       -0.020     -0.010
4 0 days 00:04:05      0.075        0.014      0.005

# Plot the price diff of the close of jpm, wells and bac only
price_diffs.plot(y=['close_jpm', 'close_wells', 'close_bac'])
plt.show()
```

![alt_text](https://github.com/heramb-joshi/DataCampDataAnalyst/blob/main/Images/Chapter%204/merge_as_of_linegraph_4.4.4.png)

>**Example2 - Using merge_asof() to create dataset**
>> The merge_asof() function can be used to create datasets where you have a table of start and stop dates, and you want to use them to create a flag in another table. You have been given gdp, which is a table of quarterly GDP values of the US during the 1980s. Additionally, the table recession has been given to you. It holds the starting date of every US recession since 1980, and the date when the recession was declared to be over. Use merge_asof() to merge the tables and create a status flag if a quarter was during a recession. Finally, to check your work, plot the data in a bar chart.

```python
# Merge gdp and recession on date using merge_asof()
gdp_recession = pd.merge_asof(gdp, recession, on='date', direction='backward') #To check the next quarter of the recession
gdp_recession

>> Output
         date       gdp econ_status
0  1979-01-01  2526.610   recession
1  1979-04-01  2591.247   recession
2  1979-07-01  2667.565   recession
3  1979-10-01  2723.883   recession
4  1980-01-01  2789.842   recession
5  1980-04-01  2797.352   recession
6  1980-07-01  2856.483      normal
7  1980-10-01  2985.557      normal
8  1981-01-01  3124.206      normal
9  1981-04-01  3162.532   recession
10 1981-07-01  3260.609   recession
11 1981-10-01  3280.818   recession
12 1982-01-01  3274.302   recession
13 1982-04-01  3331.972      normal
14 1982-07-01  3366.322      normal
15 1982-10-01  3402.561      normal
16 1983-01-01  3473.413      normal
17 1983-04-01  3578.848      normal
18 1983-07-01  3689.179      normal
19 1983-10-01  3794.706      normal
20 1984-01-01  3908.054      normal
21 1984-04-01  4009.601      normal
22 1984-07-01  4084.250      normal
23 1984-10-01  4148.551      normal

# Create a list based on the row value of gdp_recession['econ_status']
is_recession = ['r' if s=='recession' else 'g' for s in gdp_recession['econ_status']]

# Plot a bar chart of gdp_recession
gdp_recession.plot(kind="bar", y="gdp", x="date", color=is_recession, rot=90)
plt.show()
```
![alt_text](https://github.com/heramb-joshi/DataCampDataAnalyst/blob/main/Images/Chapter%204/merge_asof_bar_4.4.4.png)


## Selecting data with .query()

>**Syntax**
```python
.query('some selection statement')
```

- Accepts an input string
  - this string is used to determine what rows are returned
  - this is similar to statement after **where** clause in **SQL** statement

```python
stocks.query('nike>= 90') #Returns all rows in stocks where nike value is greater than 90

stocks.query('nike> 90 and disney < 140')

stocks.query('nike> 90 or disney < 140')

stocks.query('stock == "disney" or (stock == "nike" and close < 90)')
```

> **Example - 1 Subsetting rows with .query() methos**
>> In this exercise, you will revisit GDP and population data for Australia and Sweden from the World Bank and expand on it using the .query() method. You'll merge the two tables and compute the GDP per capita. Afterwards, you'll use the .query() method to sub-select the rows and create a plot. Recall that you will need to merge on multiple columns in the proper order.

```python
# Merge gdp and pop on date and country with fill
gdp_pop = pd.merge_ordered(gdp, pop, how="left", on=["country", "date"], fill_method='ffill')

# Add a column named gdp_per_capita to gdp_pop that divides the gdp by pop
gdp_pop['gdp_per_capita'] = gdp_pop['gdp']/gdp_pop['pop']

# Pivot table of gdp_per_capita, where index is date and columns is country
gdp_pivot = gdp_pop.pivot_table('gdp_per_capita', index='date', columns='country')

# Select dates equal to or greater than 1991-01-01
recent_gdp_pop = gdp_pivot.query('date >= "1991-01-01"')

# Plot recent_gdp_pop
recent_gdp_pop.plot(rot=90)
plt.show()
```

![alt_text](https://github.com/heramb-joshi/DataCampDataAnalyst/blob/main/Images/Chapter%204/merge_ordered_bar_4.4.4.png)

## Reshaping data with .melt() method

This method Will unpivot table from wide to long format

![alt_text](https://github.com/heramb-joshi/DataCampDataAnalyst/blob/main/Images/Chapter%204/melt_method_4.4.4.png)

> **Syntax**
```python
social_fin_tall = social_fin.melt(id_vars=['financial','company']) 
#id vars are the unque identifiers of the rows or the coluns in the original dataset we do not want to change

social_fin_tall = social_fin.melt(id_vars=['financial','company'], value_vars=['2018','2017']) #create value variables only for 2017 and 2018

social_fin_tall = social_fin.melt(id_vars=['financial','company'], 
				value_vars=['2018','2017'],
				var_name=['year'], value_name='dollars') 
```
> **Example 1 - Using .melt() to reshape government data**
>> The US Bureau of Labor Statistics (BLS) often provides data series in an easy-to-read format - it has a separate column for each month, and each year is a different row. Unfortunately, this wide format makes it difficult to plot this information over time. In this exercise, you will reshape a table of US unemployment rate data from the BLS into a form you can plot using .melt(). You will need to add a date column to the table and sort by it to plot the data correctly.

```python
# unpivot everything besides the year column
ur_tall = ur_wide.melt(id_vars=['year'], var_name=['month'], value_name='unempl_rate')
ur_tall.head()

# Create a date column using the month and year columns of ur_tall
ur_tall['date'] = pd.to_datetime(ur_tall['year'] + '-' + ur_tall['month'])

# Sort ur_tall by date in ascending order
ur_sorted = ur_tall.sort_values('date')
ur_sorted
# Plot the unempl_rate by date
ur_sorted.plot(x='date', y='unempl_rate', kind='line')
plt.show()
```
![alt_text](https://github.com/heramb-joshi/DataCampDataAnalyst/blob/main/Images/Chapter%204/melt_method_example_4.4.4.png)

> **Example 2 - Using .melt() for stocks vs bond performance**
>> It is widespread knowledge that the price of bonds is inversely related to the price of stocks. In this last exercise, you'll review many of the topics in this chapter to confirm this. You have been given a table of percent change of the US 10-year treasury bond price. It is in a wide format where there is a separate column for each year. You will need to use the .melt() method to reshape this table.

>> Additionally, you will use the .query() method to filter out unneeded data. You will merge this table with a table of the percent change of the Dow Jones Industrial stock index price. Finally, you will plot data.

```python
# Use melt on ten_yr, unpivot everything besides the metric column
bond_perc = ten_yr.melt(id_vars=['metric'], var_name='date', value_name='close')

# Use query on bond_perc to select only the rows where metric=close
bond_perc_close = bond_perc.query('metric == "close"')

# Merge (ordered) dji and bond_perc_close on date with an inner join
dow_bond = pd.merge_ordered(dji, bond_perc_close, on='date', how='inner', suffixes=('_dow','_bond'))
dow_bond

# Plot only the close_dow and close_bond columns
dow_bond.plot(y=['close_dow','close_bond'], x='date', rot=90)
plt.show()
```

![alt_text](https://github.com/heramb-joshi/DataCampDataAnalyst/blob/main/Images/Chapter%204/melt_method_example_4.4.5.png)
